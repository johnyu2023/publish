import{_ as e,a as t}from"./chunks/diff-between-gen-and-analytical.Dno_Pmk9.js";import{_ as i,C as r,c as u,o,G as c,w as m,j as l,a}from"./chunks/framework.BX-G93LU.js";const p="/publish/assets/ai-tool-packages.BbIdoZ4Y.png",v=JSON.parse('{"title":"分析式AI 01","description":"AI 用于分析数据、识别模式、支持决策等分析性任务","frontmatter":{"title":"分析式AI 01","description":"AI 用于分析数据、识别模式、支持决策等分析性任务","date":"2025-09-17T00:00:00.000Z","tags":["Analytical AI"]},"headers":[],"relativePath":"ai/analytical-ai-01.md","filePath":"ai/analytical-ai-01.md"}'),h={name:"ai/analytical-ai-01.md"};function d(g,s,b,x,y,P){const n=r("BlogPost");return o(),u("div",null,[c(n,null,{default:m(()=>s[0]||(s[0]=[l("img",{src:e,alt:"analytical-ai"},null,-1),l("h2",{id:"分析式ai与生成式ai",tabindex:"-1"},[l("code",null,"分析式AI"),a("与"),l("code",null,"生成式AI"),a(),l("a",{class:"header-anchor",href:"#分析式ai与生成式ai","aria-label":'Permalink to "`分析式AI`与`生成式AI`"'},"​")],-1),l("ul",null,[l("li",null,"分析式AI - (Analytical AI)"),l("li",null,"生成式AI - (Generative AI)")],-1),l("h3",{id:"二者的对比",tabindex:"-1"},[a("二者的对比 "),l("a",{class:"header-anchor",href:"#二者的对比","aria-label":'Permalink to "二者的对比"'},"​")],-1),l("img",{src:t,alt:"diff"},null,-1),l("ul",null,[l("li",null,"如果公司的数据资产主要是文字、图像或影片之类的非结构化内容，就应优先采用生成式AI。"),l("li",null,"如果公司的数据大部分都是结构化、数字化，就应偏重分析式AI。"),l("li",null,"生成式AI可以通过内容制作带来回报；"),l("li",null,"分析式AI通常运用预测模型来预测需求、做出数据驱动的决策，从而带来更优异的经济回报。")],-1),l("h3",{id:"二者结合使用",tabindex:"-1"},[a("二者结合使用 "),l("a",{class:"header-anchor",href:"#二者结合使用","aria-label":'Permalink to "二者结合使用"'},"​")],-1),l("ul",null,[l("li",null,"企业内部，做的很多决策，不能用生成式 AI 来分析数据，很不稳定，不可信任，只能用机器学习，统计分析。"),l("li",null,"用生成式 AI 来生成机器学习的算法，用生成的算法来进行数据分析。"),l("li",null,"分析式 AI 来分析数据，更稳定、可靠，并且可溯源、可解释。")],-1),l("h2",{id:"机器学习的模型",tabindex:"-1"},[a("机器学习的模型 "),l("a",{class:"header-anchor",href:"#机器学习的模型","aria-label":'Permalink to "机器学习的模型"'},"​")],-1),l("h3",{id:"数据集",tabindex:"-1"},[a("数据集 "),l("a",{class:"header-anchor",href:"#数据集","aria-label":'Permalink to "数据集"'},"​")],-1),l("ul",null,[l("li",null,"国外的 kaggle , 有很多的数据集，都可以用来训练模型。"),l("li",null,"国内的 阿里云的天池 , 有很多的数据集，都可以用来训练模型。")],-1),l("h3",{id:"机器学习的十大经典模型-kaggle-上投票选出来的模型",tabindex:"-1"},[a("机器学习的十大经典模型 - kaggle 上投票选出来的模型 "),l("a",{class:"header-anchor",href:"#机器学习的十大经典模型-kaggle-上投票选出来的模型","aria-label":'Permalink to "机器学习的十大经典模型 - kaggle 上投票选出来的模型"'},"​")],-1),l("ul",null,[l("li",null,"分类算法：C4.5，朴素贝叶斯（Naive Bayes），SVM，KNN，Adaboost，CART"),l("li",null,"聚类算法：K-Means，EM"),l("li",null,"关联分析：Apriori"),l("li",null,"连接分析：PageRank")],-1),l("h3",{id:"经典模型的类型解释",tabindex:"-1"},[a("经典模型的类型解释 "),l("a",{class:"header-anchor",href:"#经典模型的类型解释","aria-label":'Permalink to "经典模型的类型解释"'},"​")],-1),l("ul",null,[l("li",null,[a("分类算法：预测一个样本属于某一类别的概率。 -- "),l("strong",null,"按标准答案预测类别"),a(" -- 御姐还是萝莉")]),l("li",null,[a("聚类算法：将样本分组，使得组内的样本相似度高，组间的样本相似度低。 -- "),l("strong",null,"按相似性自动分组"),a(" -- 自动分组找规律")]),l("li",null,"关联分析：发现数据中隐藏的关联规则。 -- 找闺蜜 -- 例：酒和尿布"),l("li",null,"链接分析：评估大型数据集中不同实体（或节点）之间的关系。-- Link Analysis Algorithm -- 找影响力")],-1),l("h3",{id:"分类算法和聚类算法的对比",tabindex:"-1"},[a("分类算法和聚类算法的对比 "),l("a",{class:"header-anchor",href:"#分类算法和聚类算法的对比","aria-label":'Permalink to "分类算法和聚类算法的对比"'},"​")],-1),l("blockquote",null,[l("p",null,[l("strong",null,"分类是有监督学习，聚类是无监督学习。")])],-1),l("h4",{id:"📌-一、定义区别",tabindex:"-1"},[a("📌 一、定义区别 "),l("a",{class:"header-anchor",href:"#📌-一、定义区别","aria-label":'Permalink to "📌 一、定义区别"'},"​")],-1),l("table",{tabindex:"0"},[l("thead",null,[l("tr",null,[l("th",null,"项目"),l("th",null,"分类算法（Classification）"),l("th",null,"聚类算法（Clustering）")])]),l("tbody",null,[l("tr",null,[l("td",null,"学习类型"),l("td",null,"有监督学习（Supervised Learning）"),l("td",null,"无监督学习（Unsupervised Learning）")]),l("tr",null,[l("td",null,"是否需要标签"),l("td",null,"需要已知标签的训练数据"),l("td",null,"不需要标签，自动发现数据结构")]),l("tr",null,[l("td",null,"目标"),l("td",null,"预测新样本属于哪个预定义类别"),l("td",null,"将相似样本自动分组，发现隐藏模式")]),l("tr",null,[l("td",null,"输出"),l("td",null,"明确的类别标签（如“猫”、“狗”）"),l("td",null,"簇编号或分组（如“组1”、“组2”）")])])],-1),l("h4",{id:"📌-二、简单举例说明",tabindex:"-1"},[a("📌 二、简单举例说明 "),l("a",{class:"header-anchor",href:"#📌-二、简单举例说明","aria-label":'Permalink to "📌 二、简单举例说明"'},"​")],-1),l("h5",{id:"✅-分类算法例子-垃圾邮件识别",tabindex:"-1"},[a("✅ 分类算法例子：垃圾邮件识别 "),l("a",{class:"header-anchor",href:"#✅-分类算法例子-垃圾邮件识别","aria-label":'Permalink to "✅ 分类算法例子：垃圾邮件识别"'},"​")],-1),l("ul",null,[l("li",null,[l("strong",null,"任务"),a("：判断一封邮件是“垃圾邮件”还是“正常邮件”。")]),l("li",null,[l("strong",null,"训练数据"),a("：已有大量邮件被人工标记为“垃圾”或“正常”。")]),l("li",null,[l("strong",null,"算法"),a("：逻辑回归、决策树、SVM等。")]),l("li",null,[l("strong",null,"过程"),a("：模型从带标签的数据中学习特征（如关键词、发件人等），然后对新邮件进行分类。")]),l("li",null,[l("strong",null,"关键"),a("：必须有“正确答案”（标签）来训练模型。")])],-1),l("h5",{id:"✅-聚类算法例子-客户分群",tabindex:"-1"},[a("✅ 聚类算法例子：客户分群 "),l("a",{class:"header-anchor",href:"#✅-聚类算法例子-客户分群","aria-label":'Permalink to "✅ 聚类算法例子：客户分群"'},"​")],-1),l("ul",null,[l("li",null,[l("strong",null,"任务"),a("：将电商平台的客户自动分成不同群体，以便精准营销。")]),l("li",null,[l("strong",null,"训练数据"),a("：只有客户的购买行为、浏览记录等，"),l("strong",null,"没有预先定义的群体标签"),a("。")]),l("li",null,[l("strong",null,"算法"),a("：K-Means、层次聚类、DBSCAN等。")]),l("li",null,[l("strong",null,"过程"),a("：算法根据客户行为的相似性，自动分成若干组（比如“高消费活跃用户”、“低频低价用户”）。")]),l("li",null,[l("strong",null,"关键"),a("：没有标签，模型自己“发现”结构。")])],-1),l("h3",{id:"聚类思想是降维",tabindex:"-1"},[a("聚类思想是降维 "),l("a",{class:"header-anchor",href:"#聚类思想是降维","aria-label":'Permalink to "聚类思想是降维"'},"​")],-1),l("ul",null,[l("li",null,"将零售转变成批发"),l("li",null,"将5万用户转化为5类用户来处理")],-1),l("h2",{id:"机器学习算法工具包",tabindex:"-1"},[a("机器学习算法工具包 "),l("a",{class:"header-anchor",href:"#机器学习算法工具包","aria-label":'Permalink to "机器学习算法工具包"'},"​")],-1),l("blockquote",null,[l("p",null,"我们都是调包侠")],-1),l("img",{src:p,alt:"sklearn"},null,-1),l("h2",{id:"朴素贝叶斯分类器",tabindex:"-1"},[a("朴素贝叶斯分类器 "),l("a",{class:"header-anchor",href:"#朴素贝叶斯分类器","aria-label":'Permalink to "朴素贝叶斯分类器"'},"​")],-1),l("blockquote",null,[l("p",null,"朴素 - 说明是一种精简的模型")],-1),l("p",null,[a("朴素贝叶斯（Naive Bayes）是一种基于贝叶斯定理的分类算法 。其核心思想是选择具有最高后验概率的类别作为预测结果 。算法名称中的“朴素”（Naive）源于一个关键假设："),l("strong",null,"即假设所有特征（或属性）之间是相互独立的，每个特征独立地对分类结果产生影响。"),a(" 这个独立性假设虽然在现实中往往不成立，但它极大地简化了模型的计算，使得算法简单且易于理解，尽管可能会牺牲一些准确性 。")],-1),l("p",null,[a("朴素贝叶斯分类器是一种"),l("code",null,"监督式机器学习算法"),a("，常用于"),l("code",null,"文本分类"),a("等任务 。它通过给定的训练集，以特征之间相互独立为前提，学习输入（特征）到输出（类别）的联合概率分布 。在实际应用中，例如文本分类，多项式朴素贝叶斯会计算每个类别下各个单词出现的条件概率，并假设单词的出现是独立的 。")],-1),l("p",null,[l("strong",null,"举例说明：")],-1),l("p",null,"一个经典的例子是医院的病人诊断 。假设早上收治了六个病人，记录了他们的症状（如打喷嚏、头痛）和职业（如护士、农夫），以及最终诊断的疾病（如感冒、过敏）。",-1),l("ul",null,[l("li",null,"病人1: 打喷嚏, 护士 -> 感冒"),l("li",null,"病人2: 打喷嚏, 农夫 -> 过敏"),l("li",null,"... (其他病人数据)")],-1),l("p",null,"现在，来了一个新病人，症状是“打喷嚏”，职业是“建筑工人”。朴素贝叶斯算法会利用训练数据计算：",-1),l("ol",null,[l("li",null,"先验概率：比如P(感冒)、P(过敏)。"),l("li",null,"条件概率：比如P(打喷嚏|感冒)、P(打喷嚏|过敏)、P(建筑工人|感冒)、P(建筑工人|过敏)。（这里假设“打喷嚏”和“职业”这两个特征相互独立 ）。"),l("li",null,"应用贝叶斯定理计算后验概率：P(感冒|打喷嚏, 建筑工人) 和 P(过敏|打喷嚏, 建筑工人)。"),l("li",null,"选择后验概率更高的疾病作为预测结果。")],-1),l("p",null,"另一个常见的应用是垃圾邮件过滤 。算法会学习垃圾邮件和正常邮件中各个词语（特征）出现的频率（条件概率），然后根据新邮件中包含的词语，计算其属于垃圾邮件和正常邮件的后验概率，从而进行分类。",-1),l("h3",{id:"朴素贝叶斯分类",tabindex:"-1"},[a("朴素贝叶斯分类 "),l("a",{class:"header-anchor",href:"#朴素贝叶斯分类","aria-label":'Permalink to "朴素贝叶斯分类"'},"​")],-1),l("ul",null,[l("li",null,"常用于文本分类，文本过滤、情感预测、推荐系统等，尤其是对于英文等语言来说，分类效果很好"),l("li",null,"准备阶段，需要确定特征属性，属性值以及label => 训练集"),l("li",null,"训练阶段，输入是特征属性和训练样本，输出是分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率"),l("li",null,"应用阶段，使用分类器对新数据进行分类")],-1),l("h2",{id:"决策树与随机森林",tabindex:"-1"},[a("决策树与随机森林 "),l("a",{class:"header-anchor",href:"#决策树与随机森林","aria-label":'Permalink to "决策树与随机森林"'},"​")],-1),l("h3",{id:"决策树",tabindex:"-1"},[a("决策树 "),l("a",{class:"header-anchor",href:"#决策树","aria-label":'Permalink to "决策树"'},"​")],-1),l("ul",null,[l("li",null,"决策树基本上就是把我们以前的经验总结出来"),l("li",null,"常见的决策树算法有C4.5、ID3和CART")],-1),l("h3",{id:"信息、熵以及信息增益",tabindex:"-1"},[a("信息、熵以及信息增益 "),l("a",{class:"header-anchor",href:"#信息、熵以及信息增益","aria-label":'Permalink to "信息、熵以及信息增益"'},"​")],-1),l("blockquote",null,[l("p",null,"“决策树”是怎么做决定的。它背后靠的是三个词：信息、熵、信息增益。")],-1),l("ul",null,[l("li",null,"在决策树构建过程中，算法会计算每个特征的信息增益，选择增益最大的那个特征来划分数据，使得划分后的子集尽可能“纯净”（熵小）。这个过程会递归进行，直到满足停止条件，最终形成一棵决策树。")],-1),l("h4",{id:"什么是-信息-——-消除-不知道",tabindex:"-1"},[a("什么是“信息”？—— 消除“不知道” "),l("a",{class:"header-anchor",href:"#什么是-信息-——-消除-不知道","aria-label":'Permalink to "什么是“信息”？—— 消除“不知道”"'},"​")],-1),l("blockquote",null,[l("p",null,"信息是用来消除随机不确定性的东西（香农）")],-1),l("ul",null,[l("li",null,"“信息”的作用就是减少你对某件事的“不知道”程度。你越不确定一件事，知道它的结果后获得的信息就越多。")],-1),l("p",null,[a("📌 "),l("strong",null,"举例")],-1),l("p",null,"假设你抛一枚硬币：",-1),l("ul",null,[l("li",null,"如果硬币是公平的（50%正面，50%反面），你完全不知道结果 → 不确定性高 → 得知结果后获得“信息量大”。"),l("li",null,"如果硬币做了手脚，99%是正面 → 你几乎能猜到结果 → 不确定性低 → 得知结果后获得“信息量小”。")],-1),l("p",null,"所以，“信息量”和“不确定性”成正比。",-1),l("hr",null,null,-1),l("h4",{id:"越不可能发生的事-信息量越大",tabindex:"-1"},[a("越不可能发生的事，信息量越大 "),l("a",{class:"header-anchor",href:"#越不可能发生的事-信息量越大","aria-label":'Permalink to "越不可能发生的事，信息量越大"'},"​")],-1),l("ul",null,[l("li",null,"对于机器学习中的决策树而言，如果带分类的事物集合可以划分为多个类别当中，则某个类（xᵢ）的信息可以定义为")],-1),l("p",{class:"katex-block"},[l("span",{class:"katex-display"},[l("span",{class:"katex"},[l("span",{class:"katex-mathml"},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[l("semantics",null,[l("mrow",null,[l("menclose",{notation:"box"},[l("mstyle",{scriptlevel:"0",displaystyle:"false"},[l("mstyle",{scriptlevel:"0",displaystyle:"false"},[l("mstyle",{scriptlevel:"0",displaystyle:"true"},[l("mrow",null,[l("mi",null,"I"),l("mo",{stretchy:"false"},"("),l("mi",null,"X"),l("mo",null,"="),l("msub",null,[l("mi",null,"x"),l("mi",null,"i")]),l("mo",{stretchy:"false"},")"),l("mo",null,"="),l("mo",null,"−"),l("msub",null,[l("mrow",null,[l("mi",null,"log"),l("mo",null,"⁡")]),l("mn",null,"2")]),l("mi",null,"P"),l("mo",{stretchy:"false"},"("),l("msub",null,[l("mi",null,"x"),l("mi",null,"i")]),l("mo",{stretchy:"false"},")")])])])])])]),l("annotation",{encoding:"application/x-tex"},"\\boxed{I(X = x_i) = -\\log_2 P(x_i)} ")])])]),l("span",{class:"katex-html","aria-hidden":"true"},[l("span",{class:"base"},[l("span",{class:"strut",style:{height:"1.68em","vertical-align":"-0.59em"}}),l("span",{class:"mord"},[l("span",{class:"vlist-t vlist-t2"},[l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"1.09em"}},[l("span",{style:{top:"-3.68em"}},[l("span",{class:"pstrut",style:{height:"3.68em"}}),l("span",{class:"boxpad"},[l("span",{class:"mord"},[l("span",{class:"mord"},[l("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"I"),l("span",{class:"mopen"},"("),l("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),l("span",{class:"mrel"},"="),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),l("span",{class:"mord"},[l("span",{class:"mord mathnormal"},"x"),l("span",{class:"msupsub"},[l("span",{class:"vlist-t vlist-t2"},[l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.3117em"}},[l("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[l("span",{class:"pstrut",style:{height:"2.7em"}}),l("span",{class:"sizing reset-size6 size3 mtight"},[l("span",{class:"mord mathnormal mtight"},"i")])])]),l("span",{class:"vlist-s"},"​")]),l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.15em"}},[l("span")])])])])]),l("span",{class:"mclose"},")"),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),l("span",{class:"mrel"},"="),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),l("span",{class:"mord"},"−"),l("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),l("span",{class:"mop"},[l("span",{class:"mop"},[a("lo"),l("span",{style:{"margin-right":"0.01389em"}},"g")]),l("span",{class:"msupsub"},[l("span",{class:"vlist-t vlist-t2"},[l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.207em"}},[l("span",{style:{top:"-2.4559em","margin-right":"0.05em"}},[l("span",{class:"pstrut",style:{height:"2.7em"}}),l("span",{class:"sizing reset-size6 size3 mtight"},[l("span",{class:"mord mtight"},"2")])])]),l("span",{class:"vlist-s"},"​")]),l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.2441em"}},[l("span")])])])])]),l("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),l("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),l("span",{class:"mopen"},"("),l("span",{class:"mord"},[l("span",{class:"mord mathnormal"},"x"),l("span",{class:"msupsub"},[l("span",{class:"vlist-t vlist-t2"},[l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.3117em"}},[l("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[l("span",{class:"pstrut",style:{height:"2.7em"}}),l("span",{class:"sizing reset-size6 size3 mtight"},[l("span",{class:"mord mathnormal mtight"},"i")])])]),l("span",{class:"vlist-s"},"​")]),l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.15em"}},[l("span")])])])])]),l("span",{class:"mclose"},")")])])])]),l("span",{style:{top:"-3.09em"}},[l("span",{class:"pstrut",style:{height:"3.68em"}}),l("span",{class:"stretchy fbox",style:{height:"1.68em","border-style":"solid","border-width":"0.04em"}})])]),l("span",{class:"vlist-s"},"​")]),l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.59em"}},[l("span")])])])])])])])])],-1),l("blockquote",null,[l("p",null,"这个公式告诉我们：一个事件发生时带来的“信息量”，等于它发生概率的负对数。越不可能发生的事，信息量越大 —— 它帮我们量化了“惊讶程度”，是整个信息论和决策树算法的基石。")],-1),l("p",null,[a("📌 "),l("strong",null,"举例")],-1),l("p",null,"设随机变量 X 表示“天气”，可能取值：",-1),l("ul",null,[l("li",null,"x₁ = “晴天”，P(x₁) = 0.6 → I(X = x₁) = -log₂(0.6) ≈ 0.74 bit"),l("li",null,"x₂ = “雨天”，P(x₂) = 0.3 → I(X = x₂) = -log₂(0.3) ≈ 1.74 bit"),l("li",null,"x₃ = “雪天”，P(x₃) = 0.1 → I(X = x₃) = -log₂(0.1) ≈ 3.32 bit")],-1),l("blockquote",null,[l("p",null,"这里 P(x₁), P(x₂), P(x₃) 就是简写，实际就是 P(X=x₁) 等。")],-1),l("p",null,"“雪天”最稀有，所以一旦发生，带来的“信息”最多。",-1),l("hr",null,null,-1),l("h4",{id:"什么是-熵-——-衡量-乱不乱",tabindex:"-1"},[a("什么是“熵”？—— 衡量“乱不乱” "),l("a",{class:"header-anchor",href:"#什么是-熵-——-衡量-乱不乱","aria-label":'Permalink to "什么是“熵”？—— 衡量“乱不乱”"'},"​")],-1),l("blockquote",null,[l("p",null,"熵 = 信息的期望值，用来度量不确定性")],-1),l("ul",null,[l("li",null,"熵（Entropy）是所有可能事件的信息量按概率加权平均，也就是“平均不确定性”。")],-1),l("p",null,[a("公式："),l("br"),l("strong",null,"H(X) = - Σ P(xi) * log(P(xi))")],-1),l("ul",null,[l("li",null,"熵越大 → 整体不确定性越高 → 越“混乱”"),l("li",null,"熵越小 → 越确定 → 越“纯净”")],-1),l("p",null,[a("📌 "),l("strong",null,"举例")],-1),l("p",null,"还是成绩分类：",-1),l("p",null,[a("情况1：P(优秀)=0.1, P(良好)=0.3, P(及格)=0.6"),l("br"),a(" → 熵 = -(0.1×log0.1 + 0.3×log0.3 + 0.6×log0.6) ≈ 1.29")],-1),l("p",null,[a("情况2：P(优秀)=0.33, P(良好)=0.33, P(及格)=0.33"),l("br"),a(" → 熵 = -3×(0.33×log0.33) ≈ 1.58")],-1),l("p",null,"→ 情况2更“平均”，不确定性更高，熵更大。",-1),l("p",null,"在决策树中，我们希望划分后的子集“熵小”——也就是类别更“纯净”，比如全是“优秀”或全是“及格”。",-1),l("hr",null,null,-1),l("h4",{id:"什么是-信息增益-——-哪个问题最能-拨开迷雾",tabindex:"-1"},[a("什么是“信息增益”？—— 哪个问题最能“拨开迷雾” "),l("a",{class:"header-anchor",href:"#什么是-信息增益-——-哪个问题最能-拨开迷雾","aria-label":'Permalink to "什么是“信息增益”？—— 哪个问题最能“拨开迷雾”"'},"​")],-1),l("blockquote",null,[l("p",null,"信息增益 = 选择特征的指标，越大越好")],-1),l("p",null,[a("✅ "),l("strong",null,"意思"),a("："),l("br"),a(" 信息增益衡量的是：用某个特征划分数据后，不确定性（熵）减少了多少。")],-1),l("p",null,[a("公式："),l("br"),l("strong",null,"信息增益 = 划分前的熵 - 划分后的加权平均熵")],-1),l("ul",null,[l("li",null,"增益越大 → 这个特征划分效果越好 → 越适合用来做决策节点")],-1),l("p",null,[a("📌 "),l("strong",null,"举例")],-1),l("p",null,"假设我们要预测学生是否“优秀”，有两个特征可选：“是否熬夜”和“是否复习”。",-1),l("ul",null,[l("li",null,"用“是否复习”划分后，熵从 1.0 降到 0.3 → 信息增益 = 0.7"),l("li",null,"用“是否熬夜”划分后，熵从 1.0 降到 0.8 → 信息增益 = 0.2")],-1),l("p",null,"→ “是否复习”信息增益更大，说明它更能帮助我们判断学生是否优秀，决策树会优先选它作为分裂节点。",-1),l("hr",null,null,-1)])),_:1,__:[0]})])}const A=i(h,[["render",d]]);export{v as __pageData,A as default};
