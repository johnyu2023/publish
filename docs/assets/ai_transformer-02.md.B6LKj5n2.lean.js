import{_ as t,C as l,c as o,o as i,G as s,w as d,j as e,a}from"./chunks/framework.CdkaGE7W.js";const _=JSON.parse('{"title":"Transformer 揭秘02","description":"Transformer 内部的实现机制","frontmatter":{"title":"Transformer 揭秘02","description":"Transformer 内部的实现机制","date":"2025-03-04T00:00:00.000Z","tags":["Transformer"]},"headers":[],"relativePath":"ai/transformer-02.md","filePath":"ai/transformer-02.md"}'),c={name:"ai/transformer-02.md"};function h(f,n,m,u,b,p){const r=l("BlogPost");return i(),o("div",null,[s(r,null,{default:d(()=>[...n[0]||(n[0]=[e("h2",{id:"基本概念",tabindex:"-1"},[a("基本概念 "),e("a",{class:"header-anchor",href:"#基本概念","aria-label":'Permalink to "基本概念"'},"​")],-1),e("h2",{id:"文字序列要解决的语义问题",tabindex:"-1"},[e("code",null,"文字序列"),a("要解决的语义问题 "),e("a",{class:"header-anchor",href:"#文字序列要解决的语义问题","aria-label":'Permalink to "`文字序列`要解决的语义问题"'},"​")],-1),e("h3",{id:"编码器-解码器",tabindex:"-1"},[a("编码器 & 解码器 "),e("a",{class:"header-anchor",href:"#编码器-解码器","aria-label":'Permalink to "编码器 & 解码器"'},"​")],-1),e("ul",null,[e("li",null,"Encoder 和 Decoder 过去都是以循环神经网络 RNN 为核心计算模块"),e("li",null,"后来进化为由注意力机制 + 神经网络 组成")],-1),e("h4",{id:"过去以rnn为核心的encoder-decoder有以下几个重要的问题",tabindex:"-1"},[a("过去以RNN为核心的Encoder Decoder有以下几个重要的问题 "),e("a",{class:"header-anchor",href:"#过去以rnn为核心的encoder-decoder有以下几个重要的问题","aria-label":'Permalink to "过去以RNN为核心的Encoder Decoder有以下几个重要的问题"'},"​")],-1),e("ul",null,[e("li",null,"1、信息丢失"),e("li",null,"2、无法处理较长句子"),e("li",null,"3、不能并行计算")],-1),e("h2",{id:"attention-注意力机制",tabindex:"-1"},[a("Attention - 注意力机制 "),e("a",{class:"header-anchor",href:"#attention-注意力机制","aria-label":'Permalink to "Attention - 注意力机制"'},"​")],-1),e("h3",{id:"注意力机制attention-qkv-的定义和计算",tabindex:"-1"},[a("注意力机制Attention - QKV 的定义和计算 "),e("a",{class:"header-anchor",href:"#注意力机制attention-qkv-的定义和计算","aria-label":'Permalink to "注意力机制Attention - QKV 的定义和计算"'},"​")],-1),e("ul",null,[e("li",null,"Q 和 K 只是为了计算相关的注意力系数"),e("li",null,"真正有价值的是 V ，最后的输出是 V 中每个元素的加权求和")],-1),e("h2",{id:"self-attention-自注意力机制",tabindex:"-1"},[a("Self Attention - 自注意力机制 "),e("a",{class:"header-anchor",href:"#self-attention-自注意力机制","aria-label":'Permalink to "Self Attention - 自注意力机制"'},"​")],-1),e("ul",null,[e("li",null,"自注意力机制 - 核心目标是做信息聚合"),e("li",null,"相关度矩阵 - 这是大模型中最耗算力的地方")],-1)])]),_:1})])}const k=t(c,[["render",h]]);export{_ as __pageData,k as default};
