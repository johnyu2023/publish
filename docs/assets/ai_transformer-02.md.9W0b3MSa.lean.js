import{_ as n}from"./chunks/transformer01.DDQTcNmh.js";import{_ as t,C as s,c as l,o as m,G as f,w as i,j as r,a as e}from"./chunks/framework.BX-G93LU.js";const N=JSON.parse('{"title":"Transformer 揭秘02","description":"Transformer 内部的实现机制","frontmatter":{"title":"Transformer 揭秘02","description":"Transformer 内部的实现机制","date":"2025-03-04T00:00:00.000Z","tags":["Transformer"]},"headers":[],"relativePath":"ai/transformer-02.md","filePath":"ai/transformer-02.md"}'),d={name:"ai/transformer-02.md"};function c(u,a,p,_,h,T){const o=s("BlogPost");return m(),l("div",null,[f(o,null,{default:i(()=>a[0]||(a[0]=[r("h2",{id:"transformer-概览",tabindex:"-1"},[e("Transformer 概览 "),r("a",{class:"header-anchor",href:"#transformer-概览","aria-label":'Permalink to "Transformer 概览"'},"​")],-1),r("h3",{id:"transformer-结构图",tabindex:"-1"},[e("Transformer 结构图 "),r("a",{class:"header-anchor",href:"#transformer-结构图","aria-label":'Permalink to "Transformer 结构图"'},"​")],-1),r("p",null,[r("img",{src:n,alt:"Transformer 结构图"})],-1),r("hr",null,null,-1),r("h3",{id:"transformer-是一种框架-framework",tabindex:"-1"},[e("Transformer 是一种框架（Framework） "),r("a",{class:"header-anchor",href:"#transformer-是一种框架-framework","aria-label":'Permalink to "Transformer 是一种框架（Framework）"'},"​")],-1),r("ul",null,[r("li",null,[e("它提供了一个通用的、可扩展的架构，这个架构定义了"),r("code",null,"处理序列数据"),e("的一种新范式，而具体的模型则是在此基础上进行的实现和改进。")]),r("li",null,[e("Transformer 框架的核心是其"),r("code",null,"自注意力（self-attention）机制"),e("，它解决了传统"),r("code",null,"循环神经网络（RNN）"),e("和"),r("code",null,"卷积神经网络（CNN）"),e("在处理长序列时遇到的问题，如长距离依赖和并行计算的限制。")])],-1)])),_:1,__:[0]})])}const P=t(d,[["render",c]]);export{N as __pageData,P as default};
