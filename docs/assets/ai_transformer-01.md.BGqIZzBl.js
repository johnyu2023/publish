import{_ as t}from"./chunks/transformer01.DDQTcNmh.js";import{_ as e,C as r,c as i,o as m,G as o,w as c,j as s,a as l}from"./chunks/framework.BX-G93LU.js";const k=JSON.parse('{"title":"Transformer 揭秘01","description":"Transformer 框架总体来说，包含哪些子任务模块，它们各自的职责是什么","frontmatter":{"title":"Transformer 揭秘01","description":"Transformer 框架总体来说，包含哪些子任务模块，它们各自的职责是什么","date":"2025-03-01T00:00:00.000Z","tags":["Transformer"]},"headers":[],"relativePath":"ai/transformer-01.md","filePath":"ai/transformer-01.md"}'),p={name:"ai/transformer-01.md"};function h(u,a,g,d,y,x){const n=r("BlogPost");return m(),i("div",null,[o(n,null,{default:c(()=>a[0]||(a[0]=[s("h2",{id:"transformer-概览",tabindex:"-1"},[l("Transformer 概览 "),s("a",{class:"header-anchor",href:"#transformer-概览","aria-label":'Permalink to "Transformer 概览"'},"​")],-1),s("h3",{id:"transformer-结构图",tabindex:"-1"},[l("Transformer 结构图 "),s("a",{class:"header-anchor",href:"#transformer-结构图","aria-label":'Permalink to "Transformer 结构图"'},"​")],-1),s("p",null,[s("img",{src:t,alt:"Transformer 结构图"})],-1),s("hr",null,null,-1),s("h3",{id:"transformer-是一种框架-framework",tabindex:"-1"},[l("Transformer 是一种框架（Framework） "),s("a",{class:"header-anchor",href:"#transformer-是一种框架-framework","aria-label":'Permalink to "Transformer 是一种框架（Framework）"'},"​")],-1),s("ul",null,[s("li",null,[l("它提供了一个通用的、可扩展的架构，这个架构定义了"),s("code",null,"处理序列数据"),l("的一种新范式，而具体的模型则是在此基础上进行的实现和改进。")]),s("li",null,[l("Transformer 框架的核心是其"),s("code",null,"自注意力（self-attention）机制"),l("，它解决了传统"),s("code",null,"循环神经网络（RNN）"),l("和"),s("code",null,"卷积神经网络（CNN）"),l("在处理长序列时遇到的问题，如长距离依赖和并行计算的限制。")])],-1),s("h3",{id:"transformer-框架的基本组件",tabindex:"-1"},[l("Transformer 框架的基本组件 "),s("a",{class:"header-anchor",href:"#transformer-框架的基本组件","aria-label":'Permalink to "Transformer 框架的基本组件"'},"​")],-1),s("ul",null,[s("li",null,[s("strong",null,"编码器（Encoder）"),l(": 负责将输入序列（如句子中的每个词）转化为一系列上下文向量。它由多个相同的层堆叠而成，每一层包含一个自注意力子层和一个前馈神经网络子层。")]),s("li",null,[s("strong",null,"解码器（Decoder）"),l(": 负责根据编码器的输出和已生成的序列来生成新的输出序列。它也由多个相同的层堆叠而成，并额外增加了一个跨注意力（cross-attention）子层，用于关注编码器的输出。")]),s("li",null,[s("strong",null,"位置编码（Positional Encoding）"),l(": 这是一个关键部分，因为自注意力机制本身不包含序列顺序信息。位置编码通过给每个词的输入向量添加位置信息，解决了这个问题。")])],-1),s("h3",{id:"transformer-具体的实现",tabindex:"-1"},[l("Transformer 具体的实现 "),s("a",{class:"header-anchor",href:"#transformer-具体的实现","aria-label":'Permalink to "Transformer 具体的实现"'},"​")],-1),s("ul",null,[s("li",null,[s("strong",null,"BERT (Bidirectional Encoder Representations from Transformers)"),l(": BERT 只使用了 Transformer 的 "),s("strong",null,"编码器部分"),l("。它的核心思想是"),s("strong",null,"双向"),l("学习上下文信息。通过训练模型来预测句子中被遮蔽（masked）的词，BERT 能够学习到语言的深层表示。它主要用于理解文本，而不是生成文本，因此在问答、情感分析和命名实体识别等任务上表现出色。")]),s("li",null,[s("strong",null,"GPT (Generative Pre-trained Transformer)"),l(": GPT 只使用了 Transformer 的 "),s("strong",null,"解码器部分"),l("。它通过"),s("strong",null,"单向"),l("学习来预测序列中的下一个词。这种架构非常适合文本生成任务，因为它在训练时只能看到当前词之前的所有词。因此，它被广泛应用于文章续写、对话生成和代码生成等领域。")]),s("li",null,[s("strong",null,"T5 (Text-to-Text Transfer Transformer)"),l(": T5 采用了完整的 "),s("strong",null,"Encoder-Decoder 架构"),l("。它的独特之处在于，它将所有自然语言处理任务都统一成“文本到文本”的形式。无论任务是翻译、摘要还是分类，模型都将输入文本转换成输出文本。例如，对于翻译任务，输入是“英文句子”，输出是“中文句子”；对于分类任务，输入是“句子”，输出是“标签名称”。")]),s("li",null,[s("strong",null,"Vision Transformer (ViT)"),l(": ViT 将 Transformer 框架应用于计算机视觉领域。它将图像切分成一系列小块，并将这些小块视为序列中的“词”。然后，它使用标准的 Transformer 编码器来处理这些图像块序列，从而进行图像分类等任务。这证明了 Transformer 框架不仅限于处理文本数据。")]),s("li",null,[s("strong",null,"Transformer-XL"),l(": 针对处理超长序列的挑战而设计。它引入了“循环”机制，允许模型在处理下一个分段时重用上一个分段的隐藏状态，从而有效捕捉更长距离的依赖关系，而不会像传统 Transformer 那样受固定上下文窗口大小的限制。")])],-1),s("h3",{id:"输出每个字的不同环节-用不同的神经网络来实现",tabindex:"-1"},[l("输出每个字的不同环节，用不同的神经网络来实现 "),s("a",{class:"header-anchor",href:"#输出每个字的不同环节-用不同的神经网络来实现","aria-label":'Permalink to "输出每个字的不同环节，用不同的神经网络来实现"'},"​")],-1),s("ul",null,[s("li",null,"Transformer 框架下，人们将输出每个字的不同环节，用不同的神经网络来实现"),s("li",null,"核心思想：将复杂的任务分解成可管理的、专门化的子任务，并用不同的神经网络模块来分别处理。"),s("li",null,[l("准确表述：在 Transformer 框架下，模型通过"),s("code",null,"多层堆叠"),l("的"),s("code",null,"自注意力机制"),l("和"),s("code",null,"前馈神经网络"),l("来处理"),s("code",null,"输入序列"),l("，并生成"),s("code",null,"输出序列"),l("。这个过程通常由一个"),s("code",null,"编码器（用于理解输入）"),l("和一个"),s("code",null,"解码器（用于生成输出）"),l("协同完成。模型输出的单位是"),s("code",null,"词元"),l(" (tokens)，而非仅仅是“字”，这些词元可以是词、子词或字符，具体取决于预训练时的"),s("code",null,"分词策略"),l("。")])],-1),s("h2",{id:"基础概念解析",tabindex:"-1"},[l("基础概念解析 "),s("a",{class:"header-anchor",href:"#基础概念解析","aria-label":'Permalink to "基础概念解析"'},"​")],-1),s("h3",{id:"向量、空间、特征",tabindex:"-1"},[l("向量、空间、特征 "),s("a",{class:"header-anchor",href:"#向量、空间、特征","aria-label":'Permalink to "向量、空间、特征"'},"​")],-1),s("ul",null,[s("li",null,[s("code",null,"特征"),l("定义了向量的维度，而"),s("code",null,"向量"),l("则生活在由这些维度所定义的"),s("code",null,"空间"),l("里")])],-1),s("h4",{id:"特征",tabindex:"-1"},[l("特征 "),s("a",{class:"header-anchor",href:"#特征","aria-label":'Permalink to "特征"'},"​")],-1),s("p",null,"特征（Features）是你用来描述一个对象或一个事件的属性。 例如，如果要描述一个人的外貌，你可以用“身高”、“体重”和“年龄”作为特征。这些特征是构成描述的基础。",-1),s("h4",{id:"向量",tabindex:"-1"},[l("向量 "),s("a",{class:"header-anchor",href:"#向量","aria-label":'Permalink to "向量"'},"​")],-1),s("p",null,"当你把这些特征组合在一起，按照一定的顺序排列时，它们就形成了一个向量（Vector）。向量是一个有序的数字列表，可以看作是对象或事件的数学表示。 例如，如果一个人的身高是180cm，体重是75kg，年龄是30岁，那么这个人就可以用向量 [180,75,30] 来表示。这个向量包含了所有用于描述这个人的特征信息。",-1),s("h4",{id:"空间",tabindex:"-1"},[l("空间 "),s("a",{class:"header-anchor",href:"#空间","aria-label":'Permalink to "空间"'},"​")],-1),s("p",null,"空间（Space）是所有可能的向量的集合。它是一个数学上的抽象概念，提供了一个框架来容纳和比较向量。 在上面的例子中，我们有“身高”、“体重”和“年龄”三个特征。每个特征可以看作是一个维度。这样，我们就创建了一个三维空间，也称为“特征空间”。在这个空间里，每一个点都代表着一个特定的人，而这个点的坐标就是由这个人的身高、体重和年龄组成的向量。",-1),s("hr",null,null,-1),s("h3",{id:"标量、向量",tabindex:"-1"},[l("标量、向量 "),s("a",{class:"header-anchor",href:"#标量、向量","aria-label":'Permalink to "标量、向量"'},"​")],-1),s("h4",{id:"标量-scalar-一个只有大小-没有方向的量",tabindex:"-1"},[l("标量 Scalar - 一个只有"),s("strong",null,"大小"),l("，没有"),s("strong",null,"方向"),l("的量 "),s("a",{class:"header-anchor",href:"#标量-scalar-一个只有大小-没有方向的量","aria-label":'Permalink to "标量 Scalar - 一个只有**大小**，没有**方向**的量"'},"​")],-1),s("h4",{id:"向量-vector-一个既有大小-又有方向的量",tabindex:"-1"},[l("向量 Vector - 一个既有"),s("strong",null,"大小"),l("，又有"),s("strong",null,"方向"),l("的量 "),s("a",{class:"header-anchor",href:"#向量-vector-一个既有大小-又有方向的量","aria-label":'Permalink to "向量 Vector - 一个既有**大小**，又有**方向**的量"'},"​")],-1),s("hr",null,null,-1),s("h3",{id:"点积、叉积",tabindex:"-1"},[l("点积、叉积 "),s("a",{class:"header-anchor",href:"#点积、叉积","aria-label":'Permalink to "点积、叉积"'},"​")],-1),s("h4",{id:"乘积-product-乘法的结果",tabindex:"-1"},[l("乘积 Product - 乘法的结果 "),s("a",{class:"header-anchor",href:"#乘积-product-乘法的结果","aria-label":'Permalink to "乘积 Product - 乘法的结果"'},"​")],-1),s("h4",{id:"点积-dot-product-点乘-scalar-product-表示两个向量相乘得到一个标量",tabindex:"-1"},[l("点积 Dot Product = 点乘 Scalar product = 表示两个"),s("code",null,"向量"),l("相乘得到一个"),s("code",null,"标量"),l(),s("a",{class:"header-anchor",href:"#点积-dot-product-点乘-scalar-product-表示两个向量相乘得到一个标量","aria-label":'Permalink to "点积 Dot Product = 点乘 Scalar product = 表示两个`向量`相乘得到一个`标量`"'},"​")],-1),s("ul",null,[s("li",null,[s("strong",null,"Dot product"),l(" 这个名称来自其运算符号——一个"),s("strong",null,"点"),l("（•），例如 A • B。")]),s("li",null,[s("strong",null,"Scalar product"),l(" 这个名称则强调运算的结果是一个"),s("strong",null,"标量"),l("（scalar）。")])],-1),s("h4",{id:"叉积-cross-product-叉乘-vector-product-表示两个向量相乘得到一个向量",tabindex:"-1"},[l("叉积 Cross Product = 叉乘 Vector product = 表示两个"),s("code",null,"向量"),l("相乘得到一个"),s("code",null,"向量"),l(),s("a",{class:"header-anchor",href:"#叉积-cross-product-叉乘-vector-product-表示两个向量相乘得到一个向量","aria-label":'Permalink to "叉积 Cross Product = 叉乘 Vector product = 表示两个`向量`相乘得到一个`向量`"'},"​")],-1),s("ul",null,[s("li",null,[s("strong",null,"Cross product"),l(" 这个名称来自其运算符号——一个"),s("strong",null,"叉"),l("（×），例如 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"A"),s("mo",null,"⋅"),s("mi",null,"B")]),s("annotation",{encoding:"application/x-tex"},"A \\cdot B")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal"},"A"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"⋅"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.05017em"}},"B")])])]),l("。")]),s("li",null,[s("strong",null,"Vector product"),l(" 这个名称则强调运算的结果是一个"),s("strong",null,"向量"),l("（vector）。")])],-1),s("h4",{id:"用途",tabindex:"-1"},[l("用途 "),s("a",{class:"header-anchor",href:"#用途","aria-label":'Permalink to "用途"'},"​")],-1),s("ul",null,[s("li",null,"点积主要用来衡量向量之间的投影关系"),s("li",null,"叉乘主要用来找到与两个向量都垂直的新向量。")],-1),s("hr",null,null,-1),s("h3",{id:"ai-中点积的用途",tabindex:"-1"},[l("AI 中"),s("code",null,"点积"),l("的用途 "),s("a",{class:"header-anchor",href:"#ai-中点积的用途","aria-label":'Permalink to "AI 中`点积`的用途"'},"​")],-1),s("p",null,"点积在AI领域，尤其是深度学习中，扮演着核心角色。它主要用于解决以下几个问题：",-1),s("h4",{id:"_1-向量相似度计算",tabindex:"-1"},[l("1. 向量相似度计算 "),s("a",{class:"header-anchor",href:"#_1-向量相似度计算","aria-label":'Permalink to "1. 向量相似度计算"'},"​")],-1),s("p",null,"点积可以衡量两个向量的相似度。在 AI 中，数据（如词语、图像特征、用户偏好等）通常被表示成高维向量。通过计算这些向量的点积，我们可以判断它们在方向上的接近程度：",-1),s("ul",null,[s("li",null,[s("strong",null,"正值"),l("：两个向量方向相近，代表它们所表示的概念或数据很相似。")]),s("li",null,[s("strong",null,"负值"),l("：两个向量方向相反，代表它们不相似。")]),s("li",null,[s("strong",null,"零"),l("：两个向量正交（夹角90度），代表它们不相关。")])],-1),s("p",null,[l("这种相似度计算在"),s("strong",null,"推荐系统"),l("中尤为常见，例如，通过计算用户向量和电影向量的点积，可以预测用户对某部电影的喜好程度。")],-1),s("h4",{id:"_2-神经网络中的权重和输入",tabindex:"-1"},[l("2. 神经网络中的权重和输入 "),s("a",{class:"header-anchor",href:"#_2-神经网络中的权重和输入","aria-label":'Permalink to "2. 神经网络中的权重和输入"'},"​")],-1),s("p",null,"在神经网络的基本单元——神经元中，点积是核心计算。每个神经元接收来自上一层神经元的输入信号（向量），并将其与自身的权重（另一个向量）进行点积运算。这个点积的结果，加上一个偏置项，构成了神经元的总输入，随后通过激活函数进行非线性转换。",-1),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mtext",null,"总输入"),s("mo",null,"="),s("mtext",null,"权重"),s("mo",null,"⋅"),s("mtext",null,"输入"),s("mo",null,"+"),s("mtext",null,"偏置")]),s("annotation",{encoding:"application/x-tex"},"\\text{总输入} = \\text{权重} \\cdot \\text{输入} + \\text{偏置} ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord text"},[s("span",{class:"mord cjk_fallback"},"总输入")]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord text"},[s("span",{class:"mord cjk_fallback"},"权重")]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"⋅"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7667em","vertical-align":"-0.0833em"}}),s("span",{class:"mord text"},[s("span",{class:"mord cjk_fallback"},"输入")]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord text"},[s("span",{class:"mord cjk_fallback"},"偏置")])])])])])],-1),s("p",null,"这个过程实际上是在寻找输入数据中与该神经元“关注”的模式最匹配的部分。点积越大，意味着输入数据与神经元的权重模式越契合。",-1),s("h4",{id:"_3-注意力机制-attention-mechanism",tabindex:"-1"},[l("3. 注意力机制（Attention Mechanism） "),s("a",{class:"header-anchor",href:"#_3-注意力机制-attention-mechanism","aria-label":'Permalink to "3. 注意力机制（Attention Mechanism）"'},"​")],-1),s("p",null,"在现代的自然语言处理（NLP）模型（如 Transformer）中，点积被广泛用于实现“注意力机制”。它通过计算查询（Query）向量和多个键（Key）向量的点积，来衡量查询与每个键之间的相关性。点积结果越大，表明该查询对该键的“注意力”越高，从而赋予其更高的权重。",-1),s("p",null,"例如，在机器翻译中，当模型翻译一个单词时，它会通过点积计算当前单词（查询）与源语言句子中所有单词（键）的相似度，以决定“关注”哪些源单词来更好地理解和翻译。",-1),s("h2",{id:"token-化-tokenization-对用户输入的内容进行初始化",tabindex:"-1"},[l("token 化（tokenization） - 对用户输入的内容进行初始化 "),s("a",{class:"header-anchor",href:"#token-化-tokenization-对用户输入的内容进行初始化","aria-label":'Permalink to "token 化（tokenization） - 对用户输入的内容进行初始化"'},"​")],-1),s("ul",null,[s("li",null,"Tokenization 本身是一个很基础的文本处理过程，它的主要目的就是将一段连续的文本分解成更小的、有意义的单元，这些单元就叫做“token”。"),s("li",null,"现代 NLP 中用于大模型的 tokenization，会通过对大量文本数据的统计和学习，来决定如何创建和分割 token，从而达到更好的压缩和表示效果。"),s("li",null,"示例中，GPT 将用户输入的的650个字，转换成大概1300个Token")],-1),s("h2",{id:"embedding-token-变成向量",tabindex:"-1"},[l("Embedding - Token 变成向量 "),s("a",{class:"header-anchor",href:"#embedding-token-变成向量","aria-label":'Permalink to "Embedding - Token 变成向量"'},"​")],-1),s("ul",null,[s("li",null,[l("示例中，GPT 将用户输入的的650个字，转换成大概1300个 "),s("code",null,"Token"),l("，然后再把1300个 Token 变成1300个 "),s("code",null,"Embedding向量"),l("，每个向量12288维")])],-1),s("h2",{id:"positional-encoding",tabindex:"-1"},[l("Positional Encoding "),s("a",{class:"header-anchor",href:"#positional-encoding","aria-label":'Permalink to "Positional Encoding"'},"​")],-1),s("ul",null,[s("li",null,"在向量中加入位置信息"),s("li",null,"示例中，上一步中的650个字转化出来的1300个向量，每个向量都只表示了一个文字，还需要加入位置信息")],-1),s("h2",{id:"encoder-decoder",tabindex:"-1"},[l("Encoder & Decoder "),s("a",{class:"header-anchor",href:"#encoder-decoder","aria-label":'Permalink to "Encoder & Decoder"'},"​")],-1),s("h3",{id:"针对中心主题词的多轮聚合",tabindex:"-1"},[l("针对中心主题词的多轮聚合 "),s("a",{class:"header-anchor",href:"#针对中心主题词的多轮聚合","aria-label":'Permalink to "针对中心主题词的多轮聚合"'},"​")],-1),s("p",null,[l("在 Transformer 架构中，Encoder 和 Decoder 的核心思想是对每个 token 进行多轮的"),s("strong",null,"聚合（aggregation）"),l("。这个聚合过程主要是通过**自注意力机制（self-attention）**实现的。")],-1),s("p",null,[l("简单来说，每个 token 都会像一个中心词一样，在每一层中与其他所有 token 进行交互，并吸收（或说聚合）它们的信息，从而得到一个更加丰富的、包含了上下文信息的"),s("strong",null,"新表示（new representation）"),l("。这个过程会重复多轮，也就是经过多层，让每个 token 的表示越来越完善。")],-1),s("p",null,"我们用一个简单的句子来举例说明这个过程，假设句子是：“我喜欢吃苹果。”",-1),s("h4",{id:"_1-encoder-的聚合过程",tabindex:"-1"},[l("1. Encoder 的聚合过程 "),s("a",{class:"header-anchor",href:"#_1-encoder-的聚合过程","aria-label":'Permalink to "1. Encoder 的聚合过程"'},"​")],-1),s("p",null,"Encoder 的主要任务是理解整个句子。它会把句子中的每个 token 都处理一遍。",-1),s("ul",null,[s("li",null,[s("p",null,[s("strong",null,"第一层自注意力（以“苹果”为例）"),l(":")]),s("ul",null,[s("li",null,[l("“苹果”会去“看”句子中的所有其他词："),s("strong",null,"“我”"),l("、"),s("strong",null,"“喜欢”"),l("、"),s("strong",null,"“吃”"),l("、"),s("strong",null,"“苹果”"),l("。")]),s("li",null,[l("“苹果”会计算它与每个词之间的"),s("strong",null,"注意力分数（attention score）"),l("。例如，它可能会发现自己和“吃”之间的关系最紧密（因为“吃”直接作用于“苹果”），和“喜欢”的关系也比较密切，而和“我”的关系稍远。")]),s("li",null,[l("然后，“苹果”会根据这些注意力分数，从“我”、“喜欢”、“吃”以及自身中"),s("strong",null,"加权聚合"),l("它们的信息。")]),s("li",null,"经过这一层处理后，“苹果”这个 token 的表示就不再是孤立的，它已经融入了“我喜欢吃”这些词的上下文信息。比如，它的新表示可能会包含“被吃的东西”这样的概念。")])]),s("li",null,[s("p",null,[s("strong",null,"第二层及后续层"),l(":")]),s("ul",null,[s("li",null,"这个过程会重复，每一层都会在前一层的基础上，让每个 token 的表示变得更丰富。"),s("li",null,"例如，在第二层，“苹果”的新表示可能会再次聚合信息。这一次，它聚合的不再是原始的词，而是第一层处理后的、已经包含了部分上下文信息的词的表示。这样，它可能学到更深层次的语义，比如“苹果”在这个句子中是“我”喜欢的、被“我”吃的东西。")])])],-1),s("p",null,"这个多层聚合的过程，就像是让每个词反复“讨论”，最终达成一个共识，使得每个词的表示都包含了整个句子的全局信息。",-1),s("hr",null,null,-1),s("h4",{id:"_2-decoder-的聚合过程",tabindex:"-1"},[l("2. Decoder 的聚合过程 "),s("a",{class:"header-anchor",href:"#_2-decoder-的聚合过程","aria-label":'Permalink to "2. Decoder 的聚合过程"'},"​")],-1),s("p",null,[l("Decoder 的任务是"),s("strong",null,"生成"),l("新的 token。它的聚合过程和 Encoder 类似，但有两点关键区别：")],-1),s("h5",{id:"_1-自注意力机制的限制",tabindex:"-1"},[l("1. 自注意力机制的限制 "),s("a",{class:"header-anchor",href:"#_1-自注意力机制的限制","aria-label":'Permalink to "1. 自注意力机制的限制"'},"​")],-1),s("p",null,[l("Decoder 在生成第 N 个 token 时，为了不“偷看”未来的词，它只能看到并聚合"),s("strong",null,"前面已生成的 token"),l("（1 到 N-1）的信息。这被称为"),s("strong",null,"带掩码的自注意力（masked self-attention）"),l("。")],-1),s("h5",{id:"_2-交叉注意力机制-cross-attention",tabindex:"-1"},[l("2. 交叉注意力机制（Cross-Attention） "),s("a",{class:"header-anchor",href:"#_2-交叉注意力机制-cross-attention","aria-label":'Permalink to "2. 交叉注意力机制（Cross-Attention）"'},"​")],-1),s("p",null,[l("这是一个非常重要的聚合步骤。Decoder 的每一层都包含一个"),s("strong",null,"交叉注意力层"),l("，这个层会把 Decoder 正在处理的 token 和 "),s("strong",null,"Encoder 的所有输出"),l("进行聚合。")],-1),s("p",null,"我们以机器翻译为例，假设我们要翻译“I like apple”到“我喜欢吃苹果”。",-1),s("ul",null,[s("li",null,"Decoder 正在生成“苹果”这个词。"),s("li",null,"在生成“苹果”之前，它已经生成了“我”、“喜欢”、“吃”。"),s("li",null,[s("strong",null,"自注意力"),l(": “苹果”会先对前面已生成的“我”、“喜欢”、“吃”进行聚合，理解它们之间的关系。")]),s("li",null,[s("strong",null,"交叉注意力"),l(": 这一步是关键。“苹果”会去“看”Encoder 输出的、已经包含了**“I like apple”整个句子上下文信息**的所有表示。 "),s("ul",null,[s("li",null,"“苹果”会计算它和“I”、“like”、“apple”这三个词的 Encoder 输出之间的注意力分数。"),s("li",null,"它可能会发现自己和“apple”这个词的原始表示最相关，和“like”次之。"),s("li",null,[l("然后，它会根据这些分数，从 Encoder 的输出中"),s("strong",null,"聚合信息"),l("。")]),s("li",null,[l("通过这个过程，Decoder 不仅理解了自己已经生成的“我喜欢吃”这些词，还"),s("strong",null,"将这些信息与源语言的“I like apple”的信息联系起来"),l("，确保生成的内容与原文匹配。")])])])],-1),s("hr",null,null,-1),s("h4",{id:"总结",tabindex:"-1"},[l("总结 "),s("a",{class:"header-anchor",href:"#总结","aria-label":'Permalink to "总结"'},"​")],-1),s("ul",null,[s("li",null,[s("strong",null,"聚合"),l("是 Transformer 的核心概念，它通过"),s("strong",null,"自注意力"),l("和"),s("strong",null,"交叉注意力"),l("实现。")]),s("li",null,[s("strong",null,"Encoder"),l(" 的每一层都让每个 token 与"),s("strong",null,"句子中的所有其他 token"),l("进行聚合，形成一个包含全局上下文的新表示。")]),s("li",null,[s("strong",null,"Decoder"),l(" 的聚合过程分两步： "),s("ol",null,[s("li",null,[s("strong",null,"带掩码的自注意力"),l(": 让当前 token 只与"),s("strong",null,"前面已生成的 token"),l("聚合。")]),s("li",null,[s("strong",null,"交叉注意力"),l(": 将当前 token 与 "),s("strong",null,"Encoder 的全部输出"),l("进行聚合，从而连接源语言和目标语言的语义。")])])])],-1),s("p",null,"这个多轮聚合的过程，就像是一场持续的“头脑风暴”，每个词都在反复地从其他词那里汲取信息，并贡献自己的信息，最终形成一个更加精准和全面的理解。",-1),s("ul",null,[s("li",null,"在实现层面，会对 中心主题词 一轮聚合，经过多轮聚合后")],-1),s("h3",{id:"简单要记住的结论",tabindex:"-1"},[l("简单要记住的结论 "),s("a",{class:"header-anchor",href:"#简单要记住的结论","aria-label":'Permalink to "简单要记住的结论"'},"​")],-1),s("ul",null,[s("li",null,"Encoder 适合做分析 做一次就停了"),s("li",null,"Decoder 适合做生成 每蹦一个字，都要算一次"),s("li",null,[l("最新的大语言模型，几乎都采用了 "),s("code",null,"Decoder Only"),l(" 的架构")])],-1),s("h2",{id:"linear-softmax",tabindex:"-1"},[l("Linear & Softmax "),s("a",{class:"header-anchor",href:"#linear-softmax","aria-label":'Permalink to "Linear & Softmax"'},"​")],-1),s("p",null,[l("在 Transformer 架构中，"),s("strong",null,"Linear"),l(" 和 "),s("strong",null,"Softmax"),l(" 是两个关键的数学操作，它们通常出现在网络的末端，特别是在"),s("strong",null,"生成任务"),l("中，如机器翻译或文本摘要。这两个环节共同协作，将模型的内部表示（representation）转化为人类可读的、有意义的输出。")],-1),s("p",null,"简单来说：",-1),s("ul",null,[s("li",null,[s("strong",null,"Linear (线性层)"),l(" 负责将复杂的内部表示，"),s("strong",null,"映射"),l("到一个巨大的、包含所有可能词汇的向量空间中。")]),s("li",null,[s("strong",null,"Softmax (归一化指数函数)"),l(" 负责将这个向量中的值，"),s("strong",null,"转换"),l("成概率分布，告诉我们每个词被选中的可能性有多大。")])],-1),s("p",null,"我们继续以一个机器翻译任务为例，假设我们正在将“I like apples”翻译成“我喜欢吃苹果”，并且 Decoder 已经处理完所有步骤，准备生成最后一个词：“苹果”。",-1),s("hr",null,null,-1),s("h3",{id:"_1-linear-层-从表示到词汇表",tabindex:"-1"},[l("1. Linear 层：从表示到词汇表 "),s("a",{class:"header-anchor",href:"#_1-linear-层-从表示到词汇表","aria-label":'Permalink to "1. Linear 层：从表示到词汇表"'},"​")],-1),s("p",null,"经过 Transformer Decoder 的层层处理后，模型已经为下一个要生成的词创建了一个非常复杂的、高维度的向量表示。这个向量包含了源语言（“I like apples”）和已生成的目标语言（“我喜欢吃”）的所有上下文信息。",-1),s("p",null,[l("这个向量本身没有直接的意义，它无法直接告诉我们下一个词是“苹果”还是“香蕉”。"),s("strong",null,"Linear 层的任务就是解决这个问题"),l("。")],-1),s("p",null,"假设我们的词汇表（vocabulary）有 50,000 个词（比如，“我”、“喜欢”、“吃”、“苹果”、“香蕉”……一直到第 50,000 个词）。",-1),s("ul",null,[s("li",null,[s("strong",null,"操作"),l("：Linear 层本质上是一个全连接层（fully connected layer），它会对 Decoder 输出的那个向量进行一次"),s("strong",null,"矩阵乘法"),l("。")]),s("li",null,[s("strong",null,"输出"),l("：它会把这个高维度的向量，"),s("strong",null,"映射"),l("成一个同样大小为 50,000 的新向量。这个新的向量被称为 "),s("strong",null,"logits 向量"),l("。")])],-1),s("p",null,[s("strong",null,"举例"),l("： 如果我们的词汇表大小是 50,000，那么 Linear 层会输出一个长度为 50,000 的向量。这个向量的每一个位置都对应词汇表中的一个词，其值（logits）代表了模型对该词的“偏好”或“倾向性”。")],-1),s("p",null,"例如，这个 logits 向量可能看起来像这样：",-1),s("ul",null,[s("li",null,"..."),s("li",null,'logits["苹果"] = 8.5'),s("li",null,"..."),s("li",null,'logits["香蕉"] = 2.1'),s("li",null,"..."),s("li",null,'logits["橘子"] = 1.5'),s("li",null,"...")],-1),s("p",null,[s("strong",null,"注意"),l("：这些值（8.5, 2.1, 1.5）本身没有直观的意义，它们可以是任何实数，甚至是负数。值越大，代表模型越“倾向于”选择这个词。")],-1),s("hr",null,null,-1),s("h3",{id:"_2-softmax-层-从偏好到概率",tabindex:"-1"},[l("2. Softmax 层：从偏好到概率 "),s("a",{class:"header-anchor",href:"#_2-softmax-层-从偏好到概率","aria-label":'Permalink to "2. Softmax 层：从偏好到概率"'},"​")],-1),s("p",null,[l("Linear 层的输出告诉我们模型的“偏好”，但这不是一个概率分布。为了让这些值变成可解释的概率，并且所有概率加起来等于 1，我们需要使用 "),s("strong",null,"Softmax 函数"),l("。")],-1),s("ul",null,[s("li",null,[s("strong",null,"操作"),l("：Softmax 会对 Linear 层输出的 logits 向量进行处理。它会先对每个值取指数（"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"e"),s("mi",null,"x")])]),s("annotation",{encoding:"application/x-tex"},"e^x")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6644em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"x")])])])])])])])])])]),l("），然后用这个值除以所有值的指数和。")]),s("li",null,[s("strong",null,"数学公式"),l("：对于第 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"i")]),s("annotation",{encoding:"application/x-tex"},"i")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6595em"}}),s("span",{class:"mord mathnormal"},"i")])])]),l(" 个词，其概率计算为："),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",null,"P"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mtext",null,"word"),s("mi",null,"i")]),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mfrac",null,[s("msup",null,[s("mi",null,"e"),s("msub",null,[s("mtext",null,"logit"),s("mi",null,"i")])]),s("mrow",null,[s("munderover",null,[s("mo",null,"∑"),s("mrow",null,[s("mi",null,"j"),s("mo",null,"="),s("mn",null,"1")]),s("mi",null,"V")]),s("msup",null,[s("mi",null,"e"),s("msub",null,[s("mtext",null,"logit"),s("mi",null,"j")])])])])]),s("annotation",{encoding:"application/x-tex"},"P(\\text{word}_i) = \\frac{e^{\\text{logit}_i}}{\\sum_{j=1}^{V} e^{\\text{logit}_j}} ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord text"},[s("span",{class:"mord"},"word")]),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"2.8332em","vertical-align":"-1.307em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.5261em"}},[s("span",{style:{top:"-2.1288em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mop"},[s("span",{class:"mop op-symbol small-op",style:{position:"relative",top:"0em"}},"∑"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.9812em"}},[s("span",{style:{top:"-2.4003em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"j"),s("span",{class:"mrel mtight"},"="),s("span",{class:"mord mtight"},"1")])])]),s("span",{style:{top:"-3.2029em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.22222em"}},"V")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.4358em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8772em"}},[s("span",{style:{top:"-3.0911em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord text mtight"},[s("span",{class:"mord mtight"},"logit")]),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2052em"}},[s("span",{style:{top:"-2.2341em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"j")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.4048em"}},[s("span")])])])])])])])])])])])])])])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.677em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8491em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord text mtight"},[s("span",{class:"mord mtight"},"logit")]),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2052em"}},[s("span",{style:{top:"-2.2341em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2659em"}},[s("span")])])])])])])])])])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.307em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})])])])])])]),l(" 其中 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"V")]),s("annotation",{encoding:"application/x-tex"},"V")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.22222em"}},"V")])])]),l(" 是词汇表的大小。")])],-1),s("p",null,[s("strong",null,"举例"),l("： 继续使用上面 Linear 层的输出。Softmax 会对这些值进行处理：")],-1),s("ol",null,[s("li",null,[l("对每个 logits 值取指数： "),s("ul",null,[s("li",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"e"),s("mn",null,"8.5")]),s("mo",null,"≈"),s("mn",null,"4914.77")]),s("annotation",{encoding:"application/x-tex"},"e^{8.5} \\approx 4914.77")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8141em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"8.5")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"4914.77")])])])]),s("li",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"e"),s("mn",null,"2.1")]),s("mo",null,"≈"),s("mn",null,"8.17")]),s("annotation",{encoding:"application/x-tex"},"e^{2.1} \\approx 8.17")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8141em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"2.1")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"8.17")])])])]),s("li",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"e"),s("mn",null,"1.5")]),s("mo",null,"≈"),s("mn",null,"4.48")]),s("annotation",{encoding:"application/x-tex"},"e^{1.5} \\approx 4.48")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8141em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"1.5")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"4.48")])])])])])]),s("li",null,"计算所有指数值的总和。"),s("li",null,[l("用每个指数值除以总和，得到概率： "),s("ul",null,[s("li",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"P"),s("mo",{stretchy:"false"},"("),s("mtext",null,'"苹果"'),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mfrac",null,[s("msup",null,[s("mi",null,"e"),s("mn",null,"8.5")]),s("mrow",null,[s("msup",null,[s("mi",null,"e"),s("mn",null,"8.5")]),s("mo",null,"+"),s("msup",null,[s("mi",null,"e"),s("mn",null,"2.1")]),s("mo",null,"+"),s("mi",{mathvariant:"normal"},"."),s("mi",{mathvariant:"normal"},"."),s("mi",{mathvariant:"normal"},".")])]),s("mo",null,"≈"),s("mn",null,"0.95")]),s("annotation",{encoding:"application/x-tex"},'P(\\text{"苹果"}) = \\frac{e^{8.5}}{e^{8.5} + e^{2.1} + ...} \\approx 0.95')])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"mopen"},"("),s("span",{class:"mord text"},[s("span",{class:"mord"},'"'),s("span",{class:"mord cjk_fallback"},"苹果"),s("span",{class:"mord"},'"')]),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.4213em","vertical-align":"-0.4033em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.0179em"}},[s("span",{style:{top:"-2.655em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7463em"}},[s("span",{style:{top:"-2.786em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"8.5")])])])])])])])]),s("span",{class:"mbin mtight"},"+"),s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7463em"}},[s("span",{style:{top:"-2.786em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"2.1")])])])])])])])]),s("span",{class:"mbin mtight"},"+"),s("span",{class:"mord mtight"},"...")])])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.394em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8913em"}},[s("span",{style:{top:"-2.931em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"8.5")])])])])])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.4033em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"0.95")])])])]),s("li",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"P"),s("mo",{stretchy:"false"},"("),s("mtext",null,'"香蕉"'),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mfrac",null,[s("msup",null,[s("mi",null,"e"),s("mn",null,"2.1")]),s("mrow",null,[s("msup",null,[s("mi",null,"e"),s("mn",null,"8.5")]),s("mo",null,"+"),s("msup",null,[s("mi",null,"e"),s("mn",null,"2.1")]),s("mo",null,"+"),s("mi",{mathvariant:"normal"},"."),s("mi",{mathvariant:"normal"},"."),s("mi",{mathvariant:"normal"},".")])]),s("mo",null,"≈"),s("mn",null,"0.01")]),s("annotation",{encoding:"application/x-tex"},'P(\\text{"香蕉"}) = \\frac{e^{2.1}}{e^{8.5} + e^{2.1} + ...} \\approx 0.01')])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"mopen"},"("),s("span",{class:"mord text"},[s("span",{class:"mord"},'"'),s("span",{class:"mord cjk_fallback"},"香蕉"),s("span",{class:"mord"},'"')]),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.4213em","vertical-align":"-0.4033em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.0179em"}},[s("span",{style:{top:"-2.655em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7463em"}},[s("span",{style:{top:"-2.786em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"8.5")])])])])])])])]),s("span",{class:"mbin mtight"},"+"),s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7463em"}},[s("span",{style:{top:"-2.786em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"2.1")])])])])])])])]),s("span",{class:"mbin mtight"},"+"),s("span",{class:"mord mtight"},"...")])])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.394em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8913em"}},[s("span",{style:{top:"-2.931em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"2.1")])])])])])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.4033em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"0.01")])])])]),s("li",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"P"),s("mo",{stretchy:"false"},"("),s("mtext",null,'"橘子"'),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mfrac",null,[s("msup",null,[s("mi",null,"e"),s("mn",null,"1.5")]),s("mrow",null,[s("msup",null,[s("mi",null,"e"),s("mn",null,"8.5")]),s("mo",null,"+"),s("msup",null,[s("mi",null,"e"),s("mn",null,"2.1")]),s("mo",null,"+"),s("mi",{mathvariant:"normal"},"."),s("mi",{mathvariant:"normal"},"."),s("mi",{mathvariant:"normal"},".")])]),s("mo",null,"≈"),s("mn",null,"0.005")]),s("annotation",{encoding:"application/x-tex"},'P(\\text{"橘子"}) = \\frac{e^{1.5}}{e^{8.5} + e^{2.1} + ...} \\approx 0.005')])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"mopen"},"("),s("span",{class:"mord text"},[s("span",{class:"mord"},'"'),s("span",{class:"mord cjk_fallback"},"橘子"),s("span",{class:"mord"},'"')]),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.4213em","vertical-align":"-0.4033em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.0179em"}},[s("span",{style:{top:"-2.655em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7463em"}},[s("span",{style:{top:"-2.786em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"8.5")])])])])])])])]),s("span",{class:"mbin mtight"},"+"),s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7463em"}},[s("span",{style:{top:"-2.786em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"2.1")])])])])])])])]),s("span",{class:"mbin mtight"},"+"),s("span",{class:"mord mtight"},"...")])])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.394em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8913em"}},[s("span",{style:{top:"-2.931em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"1.5")])])])])])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.4033em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≈"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"0.005")])])])])])])],-1),s("p",null,[l("最终，Softmax 层的输出是一个"),s("strong",null,"概率分布"),l("，告诉我们模型预测下一个词是每个词的几率。在这个例子中，模型预测下一个词是“苹果”的几率高达 95%。")],-1),s("p",null,"最后，模型会根据这个概率分布做出决策，通常是选择概率最高的词（“苹果”），然后将其作为下一个生成的 token，并继续下一个循环。",-1),s("p",null,[l("总结来说，"),s("strong",null,"Linear 层"),l("负责将模型的内部理解"),s("strong",null,"投射"),l("到词汇表中，而 "),s("strong",null,"Softmax 层"),l("则将这种投射结果"),s("strong",null,"转化为"),l("清晰的、可选择的概率，完成了从抽象表示到具体词汇的最终转换。")],-1)])),_:1,__:[0]})])}const v=e(p,[["render",h]]);export{k as __pageData,v as default};
