<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>神经网络 揭秘02 | AI时代的技术分享</title>
    <meta name="description" content="深度学习中，CNN 和 RNN 是两种非常重要且功能强大的神经网络架构。它们各自被设计用来处理不同类型的数据。">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/publish/assets/style.Cpi6_9L7.css" as="style">
    <link rel="preload stylesheet" href="/publish/vp-icons.css" as="style">
    
    <script type="module" src="/publish/assets/app.D-P-VUl_.js"></script>
    <link rel="preload" href="/publish/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/publish/assets/chunks/framework.BX-G93LU.js">
    <link rel="modulepreload" href="/publish/assets/chunks/theme.2GPRGRkH.js">
    <link rel="modulepreload" href="/publish/assets/chunks/katex.ChWnQ-fc.js">
    <link rel="modulepreload" href="/publish/assets/chunks/dagre-2BBEFEWP.zmzHiKl6.js">
    <link rel="modulepreload" href="/publish/assets/chunks/c4Diagram-AAMF2YG6.D1VEPMqj.js">
    <link rel="modulepreload" href="/publish/assets/chunks/flowDiagram-THRYKUMA.g_Y184Xc.js">
    <link rel="modulepreload" href="/publish/assets/chunks/erDiagram-HZWUO2LU.ZvUSoU9E.js">
    <link rel="modulepreload" href="/publish/assets/chunks/gitGraphDiagram-OJR772UL.o9rB2Oeo.js">
    <link rel="modulepreload" href="/publish/assets/chunks/ganttDiagram-WV7ZQ7D5.Cf02k4Ny.js">
    <link rel="modulepreload" href="/publish/assets/chunks/infoDiagram-6WOFNB3A.D9j0HeAI.js">
    <link rel="modulepreload" href="/publish/assets/chunks/pieDiagram-DBDJKBY4.BgEB9BL5.js">
    <link rel="modulepreload" href="/publish/assets/chunks/quadrantDiagram-YPSRARAO.-xE3YBU7.js">
    <link rel="modulepreload" href="/publish/assets/chunks/xychartDiagram-FDP5SA34.xhgCYcDI.js">
    <link rel="modulepreload" href="/publish/assets/chunks/requirementDiagram-EGVEC5DT.haa7z4jR.js">
    <link rel="modulepreload" href="/publish/assets/chunks/sequenceDiagram-WFGC7UMF.us1bVxcD.js">
    <link rel="modulepreload" href="/publish/assets/chunks/classDiagram-3BZAVTQC.BchuiwJj.js">
    <link rel="modulepreload" href="/publish/assets/chunks/classDiagram-v2-QTMF73CY.BchuiwJj.js">
    <link rel="modulepreload" href="/publish/assets/chunks/stateDiagram-UUKSUZ4H.rqMt1UO2.js">
    <link rel="modulepreload" href="/publish/assets/chunks/stateDiagram-v2-EYPG3UTE.9W2x8_dz.js">
    <link rel="modulepreload" href="/publish/assets/chunks/journeyDiagram-FFXJYRFH.DRAwSCqy.js">
    <link rel="modulepreload" href="/publish/assets/chunks/timeline-definition-3HZDQTIS.Cp_uhK0S.js">
    <link rel="modulepreload" href="/publish/assets/chunks/mindmap-definition-LNHGMQRG.DPsMdmGn.js">
    <link rel="modulepreload" href="/publish/assets/chunks/kanban-definition-KOZQBZVT.CgS0u0IB.js">
    <link rel="modulepreload" href="/publish/assets/chunks/sankeyDiagram-HRAUVNP4.CmxE7wCt.js">
    <link rel="modulepreload" href="/publish/assets/chunks/diagram-GUPCWM2R.D6FYRNin.js">
    <link rel="modulepreload" href="/publish/assets/chunks/diagram-RP2FKANI.BYLRubw4.js">
    <link rel="modulepreload" href="/publish/assets/chunks/blockDiagram-ZYB65J3Q.CqkkD29z.js">
    <link rel="modulepreload" href="/publish/assets/chunks/architectureDiagram-KFL7JDKH.DCZpTYhq.js">
    <link rel="modulepreload" href="/publish/assets/chunks/diagram-4IRLE6MV.BlB1WS5p.js">
    <link rel="modulepreload" href="/publish/assets/chunks/virtual_mermaid-config.CQTEIV6y.js">
    <link rel="modulepreload" href="/publish/assets/ai_neural-network-02.md.B5rVDOgc.lean.js">
    <link rel="alternate" type="application/rss+xml" href="/publish/rss.xml" title="RSS Feed for AI时代的技术分享">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/publish/" data-v-1168a8e4><!--[--><!--]--><!----><span data-v-1168a8e4>AI时代开发之旅</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/publish/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/publish/list" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>文章</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/publish/about" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>关于</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/johnyu2023" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><a class="VPSocialLink no-icon" href="/publish/rss.xml" aria-label="rss" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-rss"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/johnyu2023" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><a class="VPSocialLink no-icon" href="/publish/rss.xml" aria-label="rss" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-rss"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--[--><!--[--><div class="fixed-article-title"></div><!--]--><!--]--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 collapsible has-active" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>学习笔记</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b3fd67f8><span class="vpi-chevron-right caret-icon" data-v-b3fd67f8></span></div></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/slm" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>小语言模型的春天到了吗</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/ai-images-01" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>AI 生图使用</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/data-cleaning" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>浅谈数据清洗</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/data-science" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>数据科学与AI应用开发的关系</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/function-calling-vs-mcp" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Function Calling vs MCP</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/coding-01" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>AI 辅助编程的注意事项</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/function-calling" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Function Calling 的原始形态</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/code" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>AI 辅助编程初体验</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/transformer-02" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Transformer 揭秘02</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/transformer-01" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Transformer 揭秘01</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/neural-network-02" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>神经网络 揭秘02</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/neural-network-01" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>神经网络 揭秘01</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/machine-learning" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>机器学习，深度学习，强化学习</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _publish_ai_neural-network-02" data-v-39a288b8><div><div class="blog-post" data-v-344e6b2a><div class="blog-post-header" data-v-344e6b2a><div class="post-title" data-v-344e6b2a><h1 data-v-344e6b2a>神经网络 揭秘02</h1></div><div class="post-meta" data-v-344e6b2a><div class="post-date" data-v-344e6b2a><span class="date-icon" data-v-344e6b2a>📅</span><time datetime="2025-02-27T00:00:00.000Z" data-v-344e6b2a>2025年02月27日</time></div></div><div class="post-description" data-v-344e6b2a>深度学习中，CNN 和 RNN 是两种非常重要且功能强大的神经网络架构。它们各自被设计用来处理不同类型的数据。</div></div><div class="post-content" data-v-344e6b2a><!--[--><h2 id="cnn-和-rnn" tabindex="-1">CNN 和 RNN <a class="header-anchor" href="#cnn-和-rnn" aria-label="Permalink to &quot;CNN 和 RNN&quot;">​</a></h2><h3 id="cnn-卷积神经网络-convolutional-neural-network" tabindex="-1">CNN - 卷积神经网络（Convolutional Neural Network） <a class="header-anchor" href="#cnn-卷积神经网络-convolutional-neural-network" aria-label="Permalink to &quot;CNN - 卷积神经网络（Convolutional Neural Network）&quot;">​</a></h3><p><strong>CNN</strong> 是一种主要用于处理类似<code>网格结构数据</code>（如图像）的神经网络。它的核心思想是通过<strong>卷积</strong>操作来提取特征。可以把它想象成一个“图像过滤器”，它能够自动识别图像中的各种模式和特征，比如边缘、纹理、形状等。</p><p><strong>工作原理：</strong></p><ul><li><strong>卷积层（Convolutional Layer）：</strong> 这是CNN的核心。它使用<strong>卷积核</strong>（一个小的矩阵）在输入数据上滑动，并执行数学运算，从而生成<strong>特征图</strong>。这个过程就像在照片上扫描寻找特定的图案。</li><li><strong>池化层（Pooling Layer）：</strong> 这一层的作用是“下采样”或“压缩”特征图，从而减少数据的维度，同时保留最重要的信息。这有助于减少计算量，并让模型对图像的位置变化不那么敏感。</li><li><strong>全连接层（Fully Connected Layer）：</strong> 在通过卷积和池化层提取特征后，这些特征会被输入到传统的全连接网络中，进行最终的分类或识别任务。</li></ul><p><strong>举例：</strong></p><p>假设你想让AI识别一张照片里是不是有一只猫。</p><ol><li><strong>输入：</strong> 你给模型一张猫的照片。</li><li><strong>卷积层：</strong> 卷积层会扫描这张照片，一层层地提取特征。第一层可能找到简单的边缘和线条；第二层可能组合这些线条，找到猫的耳朵、眼睛、鼻子等部位的形状；更深层则可能识别出“一张猫脸”这个复杂的概念。</li><li><strong>池化层：</strong> 再次压缩这些特征，让模型能更好地泛化。即使猫的位置稍微移动，模型也能认出来。</li><li><strong>全连接层：</strong> 将所有提取的特征综合起来，最后判断这张图片有99%的可能性是一只猫。</li></ol><p><strong>主要应用：</strong> 图像识别、目标检测、人脸识别等。</p><hr><h3 id="rnn-循环神经网络-recurrent-neural-network" tabindex="-1">RNN - 循环神经网络（Recurrent Neural Network） <a class="header-anchor" href="#rnn-循环神经网络-recurrent-neural-network" aria-label="Permalink to &quot;RNN - 循环神经网络（Recurrent Neural Network）&quot;">​</a></h3><p><strong>RNN</strong> 是一种专门用来处理<code>序列数据</code>的神经网络。与CNN不同，它的独特之处在于拥有一个“记忆”或<strong>循环</strong>结构，能将上一步的输出作为下一步的输入。这使得它非常适合处理具有时间或顺序关系的数据，比如文本、语音等。</p><p><strong>工作原理：</strong></p><ul><li><strong>循环结构：</strong> RNN会根据当前输入和之前的“记忆”（上一步的隐藏状态），来生成当前输出。这个过程不断循环，使得它能理解整个序列的上下文。</li></ul><p><strong>举例：</strong></p><p>假设你想用AI来写一个句子。</p><ol><li><strong>输入：</strong> 你输入第一个词，“我”。</li><li><strong>RNN：</strong> RNN根据“我”这个词，并考虑到它在句子开头的这个位置，输出一个预测词，比如“喜欢”。同时，它会更新自己的“记忆”。</li><li><strong>循环：</strong> 接下来，RNN将“我喜欢”这个信息作为新的输入，再预测下一个词，比如“学习”。</li><li><strong>重复：</strong> 这个过程会一直重复，直到生成一个完整的、符合语法的句子：“我喜欢学习编程。”</li></ol><p><strong>主要应用：</strong> 自然语言处理（如机器翻译、文本生成、情感分析）、语音识别、股票价格预测等。</p><hr><p><strong>总结一下：</strong></p><ul><li><strong>CNN</strong> 擅长处理<strong>空间数据</strong>（如图片），通过识别局部特征来理解全局。</li><li><strong>RNN</strong> 擅长处理<strong>时间序列数据</strong>（如文本），通过利用“记忆”来理解上下文和顺序。</li></ul><p>这两种网络架构各有侧重，但有时也会结合使用，比如在视频分析中，用CNN来处理每一帧的图像，再用RNN来理解帧与帧之间的时序关系。</p><p>希望能帮助你更好地理解！如果你还有其他问题，随时可以问我。</p><h2 id="rnn-改进版本-lstm-和-gru" tabindex="-1">RNN 改进版本 - LSTM 和 GRU <a class="header-anchor" href="#rnn-改进版本-lstm-和-gru" aria-label="Permalink to &quot;RNN 改进版本 - LSTM 和 GRU&quot;">​</a></h2><p>LSTM（长短期记忆网络）和 GRU（门控循环单元）都是为了解决传统 RNN 在处理长序列时遇到的梯度消失问题而设计的改进版本。它们通过引入“门控”机制来选择性地记住或遗忘信息，从而更好地捕捉长距离的依赖关系。</p><h3 id="长短期记忆网络-lstm" tabindex="-1">长短期记忆网络（LSTM） <a class="header-anchor" href="#长短期记忆网络-lstm" aria-label="Permalink to &quot;长短期记忆网络（LSTM）&quot;">​</a></h3><p>LSTM 引入了“细胞状态”（Cell State）这一概念，它就像一条信息高速公路，贯穿整个网络，携带旧信息流向新信息。这条高速公路上有三个“门”来控制信息的流入和流出，从而决定哪些信息被保留，哪些被丢弃。</p><p>LSTM 的三个门分别是：</p><ol><li><strong>遗忘门（Forget Gate）</strong>：决定要从细胞状态中丢弃哪些信息。它会读取上一时刻的隐藏状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> 和当前输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，然后输出一个 0 到 1 之间的数值向量，其中 1 表示“完全保留”，0 表示“完全丢弃”。</li><li><strong>输入门（Input Gate）</strong>：决定要将哪些新信息存入细胞状态。它会先用一个 Sigmoid 函数来决定要更新哪些值，然后用一个 Tanh 函数来创建新的候选值向量，最后将两者相乘，更新细胞状态。</li><li><strong>输出门（Output Gate）</strong>：决定当前隐藏状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的输出。它会先用一个 Sigmoid 函数来决定哪些细胞状态信息会作为输出，再用一个 Tanh 函数处理细胞状态，最后将两者相乘得到最终的输出。</li></ol><p><strong>用一个例子说明 LSTM 的工作原理：</strong></p><p>假设你正在处理一个句子：“我的狗很聪明，<strong>它</strong>可以帮我拿报纸。”</p><p>当模型读到“狗”时，它会学习到“狗”是句子的一个重要主体。当模型继续读到“聪明”时，它会更新细胞状态，强化“狗”的特征。</p><p>接着，当模型读到“它”这个词时：</p><ul><li><strong>遗忘门</strong>会判断，为了正确理解“它”指的是什么，不能忘记“狗”这个信息，所以它会给“狗”相关的记忆分配较高的权重（接近1），让信息继续传递。</li><li><strong>输入门</strong>会判断，“它”这个词本身包含的信息并不多，所以输入门会分配较低的权重，不让当前输入对细胞状态产生太大影响。</li><li><strong>输出门</strong>会根据新的细胞状态来决定输出，这个输出会包含“狗”的信息，以便模型在后续的词（如“拿报纸”）中能正确地将动作和“狗”联系起来。</li></ul><p>通过这种方式，LSTM 能够有效地记住“狗”这个关键信息，直到“它”出现，并正确地将“它”与“狗”联系起来，避免了长距离依赖问题。</p><hr><h3 id="门控循环单元-gru" tabindex="-1">门控循环单元（GRU） <a class="header-anchor" href="#门控循环单元-gru" aria-label="Permalink to &quot;门控循环单元（GRU）&quot;">​</a></h3><p>GRU 是 LSTM 的一个简化版本，由两个门控组成，而不是三个。它将 LSTM 的遗忘门和输入门合并成了一个<strong>更新门</strong>（Update Gate），并且将细胞状态和隐藏状态合并。</p><p>GRU 的两个门分别是：</p><ol><li><strong>更新门（Update Gate）</strong>：决定要保留多少过去的信息，以及要加入多少新的信息。这个门的功能与 LSTM 的遗忘门和输入门类似，但更加紧凑。</li><li><strong>重置门（Reset Gate）</strong>：决定忽略多少过去的隐藏状态信息。它允许模型“忘记”与当前输入不相关的信息。</li></ol><p><strong>用一个例子说明 GRU 的工作原理：</strong></p><p>我们仍然用之前的句子：“我的狗很聪明，<strong>它</strong>可以帮我拿报纸。”</p><p>当模型读到“它”这个词时：</p><ul><li><strong>更新门</strong>会判断，为了正确理解“它”，之前关于“狗”的信息非常重要，所以它会分配一个较高的权重来保留这部分信息，同时给当前输入（“它”）分配一个较小的权重，以避免过度更新。</li><li><strong>重置门</strong>会判断，句子的主语是“狗”，之前的“我”和“的”等词对理解当前上下文的重要性较低，所以它会分配一个较低的权重来“重置”或忽略这些信息。</li></ul><h3 id="lstm-与-gru-的对比" tabindex="-1">LSTM 与 GRU 的对比 <a class="header-anchor" href="#lstm-与-gru-的对比" aria-label="Permalink to &quot;LSTM 与 GRU 的对比&quot;">​</a></h3><table tabindex="0"><thead><tr><th style="text-align:left;">特征</th><th style="text-align:left;">LSTM</th><th style="text-align:left;">GRU</th></tr></thead><tbody><tr><td style="text-align:left;"><strong>门控数量</strong></td><td style="text-align:left;">3 个（遗忘门、输入门、输出门）</td><td style="text-align:left;">2 个（更新门、重置门）</td></tr><tr><td style="text-align:left;"><strong>内部状态</strong></td><td style="text-align:left;">两个独立状态（细胞状态和隐藏状态）</td><td style="text-align:left;">一个状态（隐藏状态）</td></tr><tr><td style="text-align:left;"><strong>复杂性</strong></td><td style="text-align:left;">更复杂，参数更多，计算量更大</td><td style="text-align:left;">更简单，参数更少，训练更快</td></tr><tr><td style="text-align:left;"><strong>性能</strong></td><td style="text-align:left;">在数据量大、序列很长的任务上表现可能更好</td><td style="text-align:left;">在小数据集上表现可能与 LSTM 相当，甚至更好</td></tr><tr><td style="text-align:left;"><strong>应用</strong></td><td style="text-align:left;">广泛用于机器翻译、语音识别等</td><td style="text-align:left;">广泛用于序列预测、文本生成等</td></tr></tbody></table><p><strong>总结：</strong></p><p>GRU 可以看作是 LSTM 的一个轻量级版本。在大多数情况下，两者的性能非常接近。如果你关心模型的训练速度和参数量，或者数据集相对较小，GRU 可能是更好的选择。如果你处理的是非常长且复杂的序列，并且对性能要求极高，LSTM 可能会有细微的优势。在实际应用中，通常会尝试这两种模型，然后选择在特定任务上表现最好的那一个。</p><!--]--></div></div></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/publish/ai/transformer-01" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Previous page</span><span class="title" data-v-e257564d>Transformer 揭秘01</span><!--]--></a></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/publish/ai/neural-network-01" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>神经网络 揭秘01</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>基于 MIT 协议发布</p><p class="copyright" data-v-e315a0ad>Copyright © 2024-present</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about.md\":\"BId1-xqT\",\"ai_ai-images-01.md\":\"enu2uWjO\",\"ai_code.md\":\"Bn5GmniY\",\"ai_coding-01.md\":\"taKI8TD6\",\"ai_data-cleaning.md\":\"fA5bHHn1\",\"ai_data-science.md\":\"D5p_c15w\",\"ai_function-calling-vs-mcp.md\":\"B6v2lU5B\",\"ai_function-calling.md\":\"BnaspBKl\",\"ai_machine-learning.md\":\"CeTxTvRq\",\"ai_neural-network-01.md\":\"KsQdN9XN\",\"ai_neural-network-02.md\":\"B5rVDOgc\",\"ai_slm.md\":\"CMES1D3o\",\"ai_transformer-01.md\":\"Dg9P7qlq\",\"ai_transformer-02.md\":\"-wOcBqh-\",\"index.md\":\"CqQIkB74\",\"list.md\":\"G8vaz5-y\",\"posts_ai-movie.md\":\"DMR5dI2D\",\"posts_deep-search.md\":\"ebvjOHfj\",\"posts_getting-started.md\":\"BZ_Ugxl5\",\"posts_github-pages.md\":\"8aKWFaTj\",\"posts_latex-test.md\":\"Bo7lLrXe\",\"posts_markdown-guide.md\":\"CJ0w_qkZ\",\"posts_ssh.md\":\"1LLn2SgJ\",\"think_ai-benefits-4-pm.md\":\"CKh516NB\",\"think_junior.md\":\"Cb67TQSf\",\"think_pm.md\":\"EjUrzUr7\",\"web_ai-design.md\":\"DnzZqLn-\",\"web_cursor-mcp-to-figma.md\":\"Cssi41Pr\",\"web_from-figma-to-mini.md\":\"wcRnRJME\",\"web_hybrid-rendering.md\":\"DnFEoU8y\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"zh-CN\",\"dir\":\"ltr\",\"title\":\"AI时代的技术分享\",\"description\":\"分享技术心得\",\"base\":\"/publish/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"siteTitle\":\"AI时代开发之旅\",\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"文章\",\"link\":\"/list\"},{\"text\":\"关于\",\"link\":\"/about\"}],\"sidebar\":{\"/ai/\":[{\"text\":\"学习笔记\",\"collapsed\":false,\"items\":[{\"text\":\"小语言模型的春天到了吗\",\"link\":\"/ai/slm\"},{\"text\":\"AI 生图使用\",\"link\":\"/ai/ai-images-01\"},{\"text\":\"浅谈数据清洗\",\"link\":\"/ai/data-cleaning\"},{\"text\":\"数据科学与AI应用开发的关系\",\"link\":\"/ai/data-science\"},{\"text\":\"Function Calling vs MCP\",\"link\":\"/ai/function-calling-vs-mcp\"},{\"text\":\"AI 辅助编程的注意事项\",\"link\":\"/ai/coding-01\"},{\"text\":\"Function Calling 的原始形态\",\"link\":\"/ai/function-calling\"},{\"text\":\"AI 辅助编程初体验\",\"link\":\"/ai/code\"},{\"text\":\"Transformer 揭秘02\",\"link\":\"/ai/transformer-02\"},{\"text\":\"Transformer 揭秘01\",\"link\":\"/ai/transformer-01\"},{\"text\":\"神经网络 揭秘02\",\"link\":\"/ai/neural-network-02\"},{\"text\":\"神经网络 揭秘01\",\"link\":\"/ai/neural-network-01\"},{\"text\":\"机器学习，深度学习，强化学习\",\"link\":\"/ai/machine-learning\"}]}],\"/posts/\":[{\"text\":\"技术文章\",\"collapsed\":false,\"items\":[{\"text\":\"AI 的深度检索\",\"link\":\"/posts/deep-search\"},{\"text\":\"AI 制作长视频实践\",\"link\":\"/posts/ai-movie\"},{\"text\":\"使用 GitHub Pages 部署静态网站\",\"link\":\"/posts/github-pages\"},{\"text\":\"LaTeX 测试页面\",\"link\":\"/posts/latex-test\"},{\"text\":\"Markdown 语法完全指南\",\"link\":\"/posts/markdown-guide\"},{\"text\":\"开始使用 VitePress 搭建技术博客\",\"link\":\"/posts/getting-started\"},{\"text\":\"ssh 原理图\",\"link\":\"/posts/ssh\"}]}],\"/think/\":[{\"text\":\"观察思考\",\"collapsed\":false,\"items\":[{\"text\":\"AI时代初级岗位是否存在\",\"link\":\"/think/junior\"},{\"text\":\"AI 给产品经理带来的帮助\",\"link\":\"/think/ai-benefits-4-pm\"},{\"text\":\"AI草莽时代的产品经理肖像分析\",\"link\":\"/think/pm\"}]}],\"/web/\":[{\"text\":\"前端开发\",\"collapsed\":false,\"items\":[{\"text\":\"Figma MCP 与 Cursor 配合使用\",\"link\":\"/web/cursor-mcp-to-figma\"},{\"text\":\"从 Figma 还原小程序代码\",\"link\":\"/web/from-figma-to-mini\"},{\"text\":\"AI时代设计开发工作流\",\"link\":\"/web/ai-design\"},{\"text\":\"web 网站的混合渲染\",\"link\":\"/web/hybrid-rendering\"}]}]},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/johnyu2023\"},{\"icon\":\"rss\",\"link\":\"/publish/rss.xml\"}],\"footer\":{\"message\":\"基于 MIT 协议发布\",\"copyright\":\"Copyright © 2024-present\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>