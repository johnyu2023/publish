<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Transformer 揭秘01 | AI时代的技术分享</title>
    <meta name="description" content="Transformer 框架总体来说，包含哪些子任务模块，它们各自的职责是什么">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/publish/assets/style.C2VwB-Fy.css" as="style">
    <link rel="preload stylesheet" href="/publish/vp-icons.css" as="style">
    
    <script type="module" src="/publish/assets/app.DJNUep3v.js"></script>
    <link rel="preload" href="/publish/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/publish/assets/chunks/framework.CdkaGE7W.js">
    <link rel="modulepreload" href="/publish/assets/chunks/theme.ojWqhCPx.js">
    <link rel="modulepreload" href="/publish/assets/chunks/katex.ChWnQ-fc.js">
    <link rel="modulepreload" href="/publish/assets/chunks/dagre-6UL2VRFP.DnEn8kjh.js">
    <link rel="modulepreload" href="/publish/assets/chunks/cose-bilkent-S5V4N54A.DsgziNg3.js">
    <link rel="modulepreload" href="/publish/assets/chunks/c4Diagram-YG6GDRKO.uEImUTjx.js">
    <link rel="modulepreload" href="/publish/assets/chunks/flowDiagram-NV44I4VS.DjwMJLuH.js">
    <link rel="modulepreload" href="/publish/assets/chunks/erDiagram-Q2GNP2WA.SKkgDt9p.js">
    <link rel="modulepreload" href="/publish/assets/chunks/gitGraphDiagram-NY62KEGX.fTXnCR_a.js">
    <link rel="modulepreload" href="/publish/assets/chunks/ganttDiagram-LVOFAZNH.BuZsJTpu.js">
    <link rel="modulepreload" href="/publish/assets/chunks/infoDiagram-F6ZHWCRC.D9i_U-T5.js">
    <link rel="modulepreload" href="/publish/assets/chunks/pieDiagram-ADFJNKIX.DZ_Rpslp.js">
    <link rel="modulepreload" href="/publish/assets/chunks/quadrantDiagram-AYHSOK5B.B2zDLyXy.js">
    <link rel="modulepreload" href="/publish/assets/chunks/xychartDiagram-PRI3JC2R.DmSKb7lp.js">
    <link rel="modulepreload" href="/publish/assets/chunks/requirementDiagram-UZGBJVZJ.CfYu06Op.js">
    <link rel="modulepreload" href="/publish/assets/chunks/sequenceDiagram-WL72ISMW.DnOpnkJy.js">
    <link rel="modulepreload" href="/publish/assets/chunks/classDiagram-2ON5EDUG.DpxWYP2d.js">
    <link rel="modulepreload" href="/publish/assets/chunks/classDiagram-v2-WZHVMYZB.DpxWYP2d.js">
    <link rel="modulepreload" href="/publish/assets/chunks/stateDiagram-FKZM4ZOC.CIjAzRq9.js">
    <link rel="modulepreload" href="/publish/assets/chunks/stateDiagram-v2-4FDKWEC3.DBJDav9Q.js">
    <link rel="modulepreload" href="/publish/assets/chunks/journeyDiagram-XKPGCS4Q.ZFvrGgJx.js">
    <link rel="modulepreload" href="/publish/assets/chunks/timeline-definition-IT6M3QCI.vFKBuu-3.js">
    <link rel="modulepreload" href="/publish/assets/chunks/mindmap-definition-VGOIOE7T.DA7H-JSd.js">
    <link rel="modulepreload" href="/publish/assets/chunks/kanban-definition-3W4ZIXB7.B2xfn5M4.js">
    <link rel="modulepreload" href="/publish/assets/chunks/sankeyDiagram-TZEHDZUN.Venej9id.js">
    <link rel="modulepreload" href="/publish/assets/chunks/diagram-S2PKOQOG.YN5XB0Jl.js">
    <link rel="modulepreload" href="/publish/assets/chunks/diagram-QEK2KX5R.CFVQ_6I0.js">
    <link rel="modulepreload" href="/publish/assets/chunks/blockDiagram-VD42YOAC.C1bkPs94.js">
    <link rel="modulepreload" href="/publish/assets/chunks/architectureDiagram-VXUJARFQ.CY1Rxlzh.js">
    <link rel="modulepreload" href="/publish/assets/chunks/diagram-PSM6KHXK.ByKwy9gD.js">
    <link rel="modulepreload" href="/publish/assets/chunks/virtual_mermaid-config.CQTEIV6y.js">
    <link rel="modulepreload" href="/publish/assets/ai_transformer-01.md.ClveA2oS.lean.js">
    <link rel="alternate" type="application/rss+xml" href="/publish/rss.xml" title="RSS Feed for AI时代的技术分享">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    <style>
      body {
        background-color: #f0f0f0 !important;
      }
      .debug-message {
        position: fixed !important;
        top: 20px !important;
        left: 20px !important;
        background-color: purple !important;
        color: white !important;
        padding: 20px !important;
        font-size: 16px !important;
        z-index: 9999 !important;
      }
      
      /* 非常明显的全局测试样式 */
      .global-test-banner {
        position: fixed !important;
        top: 100px !important;
        left: 0 !important;
        right: 0 !important;
        background-color: yellow !important;
        color: black !important;
        padding: 30px !important;
        font-size: 24px !important;
        font-weight: bold !important;
        text-align: center !important;
        z-index: 9998 !important;
      }
    </style>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/publish/" data-v-1168a8e4><!--[--><!--]--><!----><span data-v-1168a8e4>AI时代开发之旅</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/publish/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/publish/list" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>文章</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/publish/about" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>关于</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/johnyu2023" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><a class="VPSocialLink no-icon" href="/publish/rss.xml" aria-label="rss" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-rss"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/johnyu2023" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><a class="VPSocialLink no-icon" href="/publish/rss.xml" aria-label="rss" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-rss"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--[--><button style="background-color:gray;color:white;border:none;border-radius:4px;padding:10px 15px;font-size:16px;cursor:pointer;width:100%;text-align:center;margin-bottom:10px;transition:background-color 0.3s;">高级导航</button><!--]--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 collapsible has-active" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>学习笔记</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b3fd67f8><span class="vpi-chevron-right caret-icon" data-v-b3fd67f8></span></div></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/time-series-analysis" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>时间序列分析</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/used-car" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>二手车预测价格大赛</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/ai-algorithms-in-different-fields" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>不同领域的 AI 算法</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/analytical-ai-01" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>分析式AI 01</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/rag01" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>RAG 01</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/vector-database" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>向量数据库</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/embeddings" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Embeddings</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/coze-01" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Coze 使用 01</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/slm" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>小语言模型的春天到了吗</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/ai-images-01" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>AI 生图使用</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/function-calling-vs-mcp" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Function Calling vs MCP</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/function-calling" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Function Calling 的原始形态</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/ai-basic-01" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>AI 基础 01</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/transformer-02" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Transformer 揭秘02</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/transformer-01" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Transformer 揭秘01</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/neural-network-02" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>神经网络 揭秘02</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/neural-network-01" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>神经网络 揭秘01</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/linear-algebra-02" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>线性代数基础 02</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/linear-algebra-01" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>线性代数基础 01</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/publish/ai/machine-learning" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>机器学习，深度学习，强化学习</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _publish_ai_transformer-01" data-v-39a288b8><div><div class="blog-post" data-v-e602dff2><div class="blog-post-header" data-v-e602dff2><div class="post-title" data-v-e602dff2><h1 data-v-e602dff2>Transformer 揭秘01</h1></div><div class="post-meta" data-v-e602dff2><div class="post-date" data-v-e602dff2><span class="date-icon" data-v-e602dff2>📅</span><time datetime="2025-03-01T00:00:00.000Z" data-v-e602dff2>2025年03月01日</time></div></div><div class="post-description" data-v-e602dff2>Transformer 框架总体来说，包含哪些子任务模块，它们各自的职责是什么</div></div><div class="post-content" data-v-e602dff2><!--[--><h2 id="transformer-概览" tabindex="-1">Transformer 概览 <a class="header-anchor" href="#transformer-概览" aria-label="Permalink to &quot;Transformer 概览&quot;">​</a></h2><h3 id="transformer-结构图" tabindex="-1">Transformer 结构图 <a class="header-anchor" href="#transformer-结构图" aria-label="Permalink to &quot;Transformer 结构图&quot;">​</a></h3><p><img src="/publish/assets/transformer01.CmYkGdyS.png" alt="Transformer 结构图"></p><hr><h3 id="transformer-是一种框架-framework" tabindex="-1">Transformer 是一种框架（Framework） <a class="header-anchor" href="#transformer-是一种框架-framework" aria-label="Permalink to &quot;Transformer 是一种框架（Framework）&quot;">​</a></h3><ul><li>它提供了一个通用的、可扩展的架构，这个架构定义了<code>处理序列数据</code>的一种新范式，而具体的模型则是在此基础上进行的实现和改进。</li><li>Transformer 框架的核心是其<code>自注意力（self-attention）机制</code>，它解决了传统<code>循环神经网络（RNN）</code>和<code>卷积神经网络（CNN）</code>在处理长序列时遇到的问题，如长距离依赖和并行计算的限制。</li></ul><h3 id="transformer-框架的基本组件" tabindex="-1">Transformer 框架的基本组件 <a class="header-anchor" href="#transformer-框架的基本组件" aria-label="Permalink to &quot;Transformer 框架的基本组件&quot;">​</a></h3><ul><li><strong>编码器（Encoder）</strong>: 负责将输入序列（如句子中的每个词）转化为一系列上下文向量。它由多个相同的层堆叠而成，每一层包含一个自注意力子层和一个前馈神经网络子层。</li><li><strong>解码器（Decoder）</strong>: 负责根据编码器的输出和已生成的序列来生成新的输出序列。它也由多个相同的层堆叠而成，并额外增加了一个跨注意力（cross-attention）子层，用于关注编码器的输出。</li><li><strong>位置编码（Positional Encoding）</strong>: 这是一个关键部分，因为自注意力机制本身不包含序列顺序信息。位置编码通过给每个词的输入向量添加位置信息，解决了这个问题。</li></ul><h3 id="transformer-具体的实现" tabindex="-1">Transformer 具体的实现 <a class="header-anchor" href="#transformer-具体的实现" aria-label="Permalink to &quot;Transformer 具体的实现&quot;">​</a></h3><ul><li><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: BERT 只使用了 Transformer 的 <strong>编码器部分</strong>。它的核心思想是<strong>双向</strong>学习上下文信息。通过训练模型来预测句子中被遮蔽（masked）的词，BERT 能够学习到语言的深层表示。它主要用于理解文本，而不是生成文本，因此在问答、情感分析和命名实体识别等任务上表现出色。</li><li><strong>GPT (Generative Pre-trained Transformer)</strong>: GPT 只使用了 Transformer 的 <strong>解码器部分</strong>。它通过<strong>单向</strong>学习来预测序列中的下一个词。这种架构非常适合文本生成任务，因为它在训练时只能看到当前词之前的所有词。因此，它被广泛应用于文章续写、对话生成和代码生成等领域。</li><li><strong>T5 (Text-to-Text Transfer Transformer)</strong>: T5 采用了完整的 <strong>Encoder-Decoder 架构</strong>。它的独特之处在于，它将所有自然语言处理任务都统一成“文本到文本”的形式。无论任务是翻译、摘要还是分类，模型都将输入文本转换成输出文本。例如，对于翻译任务，输入是“英文句子”，输出是“中文句子”；对于分类任务，输入是“句子”，输出是“标签名称”。</li><li><strong>Vision Transformer (ViT)</strong>: ViT 将 Transformer 框架应用于计算机视觉领域。它将图像切分成一系列小块，并将这些小块视为序列中的“词”。然后，它使用标准的 Transformer 编码器来处理这些图像块序列，从而进行图像分类等任务。这证明了 Transformer 框架不仅限于处理文本数据。</li><li><strong>Transformer-XL</strong>: 针对处理超长序列的挑战而设计。它引入了“循环”机制，允许模型在处理下一个分段时重用上一个分段的隐藏状态，从而有效捕捉更长距离的依赖关系，而不会像传统 Transformer 那样受固定上下文窗口大小的限制。</li></ul><h3 id="输出每个字的不同环节-用不同的神经网络来实现" tabindex="-1">输出每个字的不同环节，用不同的神经网络来实现 <a class="header-anchor" href="#输出每个字的不同环节-用不同的神经网络来实现" aria-label="Permalink to &quot;输出每个字的不同环节，用不同的神经网络来实现&quot;">​</a></h3><ul><li>Transformer 框架下，人们将输出每个字的不同环节，用不同的神经网络来实现</li><li>核心思想：将复杂的任务分解成可管理的、专门化的子任务，并用不同的神经网络模块来分别处理。</li><li>准确表述：在 Transformer 框架下，模型通过<code>多层堆叠</code>的<code>自注意力机制</code>和<code>前馈神经网络</code>来处理<code>输入序列</code>，并生成<code>输出序列</code>。这个过程通常由一个<code>编码器（用于理解输入）</code>和一个<code>解码器（用于生成输出）</code>协同完成。模型输出的单位是<code>词元</code> (tokens)，而非仅仅是“字”，这些词元可以是词、子词或字符，具体取决于预训练时的<code>分词策略</code>。</li></ul><h2 id="基础概念解析" tabindex="-1">基础概念解析 <a class="header-anchor" href="#基础概念解析" aria-label="Permalink to &quot;基础概念解析&quot;">​</a></h2><h3 id="向量、空间、特征" tabindex="-1">向量、空间、特征 <a class="header-anchor" href="#向量、空间、特征" aria-label="Permalink to &quot;向量、空间、特征&quot;">​</a></h3><ul><li><code>特征</code>定义了向量的维度，而<code>向量</code>则生活在由这些维度所定义的<code>空间</code>里</li></ul><h4 id="特征" tabindex="-1">特征 <a class="header-anchor" href="#特征" aria-label="Permalink to &quot;特征&quot;">​</a></h4><p>特征（Features）是你用来描述一个对象或一个事件的属性。 例如，如果要描述一个人的外貌，你可以用“身高”、“体重”和“年龄”作为特征。这些特征是构成描述的基础。</p><h4 id="向量" tabindex="-1">向量 <a class="header-anchor" href="#向量" aria-label="Permalink to &quot;向量&quot;">​</a></h4><p>当你把这些特征组合在一起，按照一定的顺序排列时，它们就形成了一个向量（Vector）。向量是一个有序的数字列表，可以看作是对象或事件的数学表示。 例如，如果一个人的身高是180cm，体重是75kg，年龄是30岁，那么这个人就可以用向量 [180,75,30] 来表示。这个向量包含了所有用于描述这个人的特征信息。</p><h4 id="空间" tabindex="-1">空间 <a class="header-anchor" href="#空间" aria-label="Permalink to &quot;空间&quot;">​</a></h4><p>空间（Space）是所有可能的向量的集合。它是一个数学上的抽象概念，提供了一个框架来容纳和比较向量。 在上面的例子中，我们有“身高”、“体重”和“年龄”三个特征。每个特征可以看作是一个维度。这样，我们就创建了一个三维空间，也称为“特征空间”。在这个空间里，每一个点都代表着一个特定的人，而这个点的坐标就是由这个人的身高、体重和年龄组成的向量。</p><hr><h3 id="点积" tabindex="-1">点积 <a class="header-anchor" href="#点积" aria-label="Permalink to &quot;点积&quot;">​</a></h3><h4 id="点积-dot-product-点乘-scalar-product-表示两个向量相乘得到一个标量" tabindex="-1">点积 Dot Product = 点乘 Scalar product = 表示两个<code>向量</code>相乘得到一个<code>标量</code> <a class="header-anchor" href="#点积-dot-product-点乘-scalar-product-表示两个向量相乘得到一个标量" aria-label="Permalink to &quot;点积 Dot Product = 点乘 Scalar product = 表示两个`向量`相乘得到一个`标量`&quot;">​</a></h4><ul><li><strong>Dot product</strong> 这个名称来自其运算符号——一个<strong>点</strong>（•），例如 A • B。</li><li><strong>Scalar product</strong> 这个名称则强调运算的结果是一个<strong>标量</strong>（scalar）。</li></ul><h4 id="叉积-cross-product-叉乘-vector-product-表示两个向量相乘得到一个向量" tabindex="-1">叉积 Cross Product = 叉乘 Vector product = 表示两个<code>向量</code>相乘得到一个<code>向量</code> <a class="header-anchor" href="#叉积-cross-product-叉乘-vector-product-表示两个向量相乘得到一个向量" aria-label="Permalink to &quot;叉积 Cross Product = 叉乘 Vector product = 表示两个`向量`相乘得到一个`向量`&quot;">​</a></h4><ul><li><strong>Cross product</strong> 这个名称来自其运算符号——一个<strong>叉</strong>（×），例如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⋅</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A \cdot B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>。</li><li><strong>Vector product</strong> 这个名称则强调运算的结果是一个<strong>向量</strong>（vector）。</li></ul><h4 id="用途" tabindex="-1">用途 <a class="header-anchor" href="#用途" aria-label="Permalink to &quot;用途&quot;">​</a></h4><ul><li>点积主要用来衡量向量之间的投影关系</li><li>叉乘主要用来找到与两个向量都垂直的新向量。</li></ul><hr><h3 id="ai-中点积的用途" tabindex="-1">AI 中<code>点积</code>的用途 <a class="header-anchor" href="#ai-中点积的用途" aria-label="Permalink to &quot;AI 中`点积`的用途&quot;">​</a></h3><p>点积在AI领域，尤其是深度学习中，扮演着核心角色。它主要用于解决以下几个问题：</p><h4 id="_1-向量相似度计算" tabindex="-1">1. 向量相似度计算 <a class="header-anchor" href="#_1-向量相似度计算" aria-label="Permalink to &quot;1. 向量相似度计算&quot;">​</a></h4><p>点积可以衡量两个向量的相似度。在 AI 中，数据（如词语、图像特征、用户偏好等）通常被表示成高维向量。通过计算这些向量的点积，我们可以判断它们在方向上的接近程度：</p><ul><li><strong>正值</strong>：两个向量方向相近，代表它们所表示的概念或数据很相似。</li><li><strong>负值</strong>：两个向量方向相反，代表它们不相似。</li><li><strong>零</strong>：两个向量正交（夹角90度），代表它们不相关。</li></ul><p>这种相似度计算在<strong>推荐系统</strong>中尤为常见，例如，通过计算用户向量和电影向量的点积，可以预测用户对某部电影的喜好程度。</p><h4 id="_2-神经网络中的权重和输入" tabindex="-1">2. 神经网络中的权重和输入 <a class="header-anchor" href="#_2-神经网络中的权重和输入" aria-label="Permalink to &quot;2. 神经网络中的权重和输入&quot;">​</a></h4><p>在神经网络的基本单元——神经元中，点积是核心计算。每个神经元接收来自上一层神经元的输入信号（向量），并将其与自身的权重（另一个向量）进行点积运算。这个点积的结果，加上一个偏置项，构成了神经元的总输入，随后通过激活函数进行非线性转换。</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>总输入</mtext><mo>=</mo><mtext>权重</mtext><mo>⋅</mo><mtext>输入</mtext><mo>+</mo><mtext>偏置</mtext></mrow><annotation encoding="application/x-tex">\text{总输入} = \text{权重} \cdot \text{输入} + \text{偏置} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord cjk_fallback">总输入</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord cjk_fallback">权重</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord cjk_fallback">输入</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord cjk_fallback">偏置</span></span></span></span></span></span></p><p>这个过程实际上是在寻找输入数据中与该神经元“关注”的模式最匹配的部分。点积越大，意味着输入数据与神经元的权重模式越契合。</p><h4 id="_3-注意力机制-attention-mechanism" tabindex="-1">3. 注意力机制（Attention Mechanism） <a class="header-anchor" href="#_3-注意力机制-attention-mechanism" aria-label="Permalink to &quot;3. 注意力机制（Attention Mechanism）&quot;">​</a></h4><p>在现代的自然语言处理（NLP）模型（如 Transformer）中，点积被广泛用于实现“注意力机制”。它通过计算查询（Query）向量和多个键（Key）向量的点积，来衡量查询与每个键之间的相关性。点积结果越大，表明该查询对该键的“注意力”越高，从而赋予其更高的权重。</p><p>例如，在机器翻译中，当模型翻译一个单词时，它会通过点积计算当前单词（查询）与源语言句子中所有单词（键）的相似度，以决定“关注”哪些源单词来更好地理解和翻译。</p><h2 id="token-化-tokenization-对用户输入的内容进行初始化" tabindex="-1">token 化（tokenization） - 对用户输入的内容进行初始化 <a class="header-anchor" href="#token-化-tokenization-对用户输入的内容进行初始化" aria-label="Permalink to &quot;token 化（tokenization） - 对用户输入的内容进行初始化&quot;">​</a></h2><ul><li>Tokenization 本身是一个很基础的文本处理过程，它的主要目的就是将一段连续的文本分解成更小的、有意义的单元，这些单元就叫做“token”。</li><li>现代 NLP 中用于大模型的 tokenization，会通过对大量文本数据的统计和学习，来决定如何创建和分割 token，从而达到更好的压缩和表示效果。</li><li>示例中，GPT 将用户输入的的650个字，转换成大概1300个Token</li></ul><h2 id="embedding-token-变成向量" tabindex="-1">Embedding - Token 变成向量 <a class="header-anchor" href="#embedding-token-变成向量" aria-label="Permalink to &quot;Embedding - Token 变成向量&quot;">​</a></h2><ul><li>示例中，GPT 将用户输入的的650个字，转换成大概1300个 <code>Token</code>，然后再把1300个 Token 变成1300个 <code>Embedding向量</code>，每个向量12288维</li></ul><h2 id="positional-encoding" tabindex="-1">Positional Encoding <a class="header-anchor" href="#positional-encoding" aria-label="Permalink to &quot;Positional Encoding&quot;">​</a></h2><ul><li>在向量中加入位置信息</li><li>示例中，上一步中的650个字转化出来的1300个向量，每个向量都只表示了一个文字，还需要加入位置信息</li></ul><h2 id="encoder-decoder" tabindex="-1">Encoder &amp; Decoder <a class="header-anchor" href="#encoder-decoder" aria-label="Permalink to &quot;Encoder &amp; Decoder&quot;">​</a></h2><h3 id="针对中心主题词的多轮聚合" tabindex="-1">针对中心主题词的多轮聚合 <a class="header-anchor" href="#针对中心主题词的多轮聚合" aria-label="Permalink to &quot;针对中心主题词的多轮聚合&quot;">​</a></h3><p>在 Transformer 架构中，Encoder 和 Decoder 的核心思想是对每个 token 进行多轮的<strong>聚合（aggregation）</strong>。这个聚合过程主要是通过**自注意力机制（self-attention）**实现的。</p><p>简单来说，每个 token 都会像一个中心词一样，在每一层中与其他所有 token 进行交互，并吸收（或说聚合）它们的信息，从而得到一个更加丰富的、包含了上下文信息的<strong>新表示（new representation）</strong>。这个过程会重复多轮，也就是经过多层，让每个 token 的表示越来越完善。</p><p>我们用一个简单的句子来举例说明这个过程，假设句子是：“我喜欢吃苹果。”</p><h4 id="_1-encoder-的聚合过程" tabindex="-1">1. Encoder 的聚合过程 <a class="header-anchor" href="#_1-encoder-的聚合过程" aria-label="Permalink to &quot;1. Encoder 的聚合过程&quot;">​</a></h4><p>Encoder 的主要任务是理解整个句子。它会把句子中的每个 token 都处理一遍。</p><ul><li><p><strong>第一层自注意力（以“苹果”为例）</strong>:</p><ul><li>“苹果”会去“看”句子中的所有其他词：<strong>“我”</strong>、<strong>“喜欢”</strong>、<strong>“吃”</strong>、<strong>“苹果”</strong>。</li><li>“苹果”会计算它与每个词之间的<strong>注意力分数（attention score）</strong>。例如，它可能会发现自己和“吃”之间的关系最紧密（因为“吃”直接作用于“苹果”），和“喜欢”的关系也比较密切，而和“我”的关系稍远。</li><li>然后，“苹果”会根据这些注意力分数，从“我”、“喜欢”、“吃”以及自身中<strong>加权聚合</strong>它们的信息。</li><li>经过这一层处理后，“苹果”这个 token 的表示就不再是孤立的，它已经融入了“我喜欢吃”这些词的上下文信息。比如，它的新表示可能会包含“被吃的东西”这样的概念。</li></ul></li><li><p><strong>第二层及后续层</strong>:</p><ul><li>这个过程会重复，每一层都会在前一层的基础上，让每个 token 的表示变得更丰富。</li><li>例如，在第二层，“苹果”的新表示可能会再次聚合信息。这一次，它聚合的不再是原始的词，而是第一层处理后的、已经包含了部分上下文信息的词的表示。这样，它可能学到更深层次的语义，比如“苹果”在这个句子中是“我”喜欢的、被“我”吃的东西。</li></ul></li></ul><p>这个多层聚合的过程，就像是让每个词反复“讨论”，最终达成一个共识，使得每个词的表示都包含了整个句子的全局信息。</p><hr><h4 id="_2-decoder-的聚合过程" tabindex="-1">2. Decoder 的聚合过程 <a class="header-anchor" href="#_2-decoder-的聚合过程" aria-label="Permalink to &quot;2. Decoder 的聚合过程&quot;">​</a></h4><p>Decoder 的任务是<strong>生成</strong>新的 token。它的聚合过程和 Encoder 类似，但有两点关键区别：</p><h5 id="_1-自注意力机制的限制" tabindex="-1">1. 自注意力机制的限制 <a class="header-anchor" href="#_1-自注意力机制的限制" aria-label="Permalink to &quot;1. 自注意力机制的限制&quot;">​</a></h5><p>Decoder 在生成第 N 个 token 时，为了不“偷看”未来的词，它只能看到并聚合<strong>前面已生成的 token</strong>（1 到 N-1）的信息。这被称为<strong>带掩码的自注意力（masked self-attention）</strong>。</p><h5 id="_2-交叉注意力机制-cross-attention" tabindex="-1">2. 交叉注意力机制（Cross-Attention） <a class="header-anchor" href="#_2-交叉注意力机制-cross-attention" aria-label="Permalink to &quot;2. 交叉注意力机制（Cross-Attention）&quot;">​</a></h5><p>这是一个非常重要的聚合步骤。Decoder 的每一层都包含一个<strong>交叉注意力层</strong>，这个层会把 Decoder 正在处理的 token 和 <strong>Encoder 的所有输出</strong>进行聚合。</p><p>我们以机器翻译为例，假设我们要翻译“I like apple”到“我喜欢吃苹果”。</p><ul><li>Decoder 正在生成“苹果”这个词。</li><li>在生成“苹果”之前，它已经生成了“我”、“喜欢”、“吃”。</li><li><strong>自注意力</strong>: “苹果”会先对前面已生成的“我”、“喜欢”、“吃”进行聚合，理解它们之间的关系。</li><li><strong>交叉注意力</strong>: 这一步是关键。“苹果”会去“看”Encoder 输出的、已经包含了**“I like apple”整个句子上下文信息**的所有表示。 <ul><li>“苹果”会计算它和“I”、“like”、“apple”这三个词的 Encoder 输出之间的注意力分数。</li><li>它可能会发现自己和“apple”这个词的原始表示最相关，和“like”次之。</li><li>然后，它会根据这些分数，从 Encoder 的输出中<strong>聚合信息</strong>。</li><li>通过这个过程，Decoder 不仅理解了自己已经生成的“我喜欢吃”这些词，还<strong>将这些信息与源语言的“I like apple”的信息联系起来</strong>，确保生成的内容与原文匹配。</li></ul></li></ul><hr><h4 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h4><ul><li><strong>聚合</strong>是 Transformer 的核心概念，它通过<strong>自注意力</strong>和<strong>交叉注意力</strong>实现。</li><li><strong>Encoder</strong> 的每一层都让每个 token 与<strong>句子中的所有其他 token</strong>进行聚合，形成一个包含全局上下文的新表示。</li><li><strong>Decoder</strong> 的聚合过程分两步： <ol><li><strong>带掩码的自注意力</strong>: 让当前 token 只与<strong>前面已生成的 token</strong>聚合。</li><li><strong>交叉注意力</strong>: 将当前 token 与 <strong>Encoder 的全部输出</strong>进行聚合，从而连接源语言和目标语言的语义。</li></ol></li></ul><p>这个多轮聚合的过程，就像是一场持续的“头脑风暴”，每个词都在反复地从其他词那里汲取信息，并贡献自己的信息，最终形成一个更加精准和全面的理解。</p><ul><li>在实现层面，会对 中心主题词 一轮聚合，经过多轮聚合后</li></ul><h3 id="简单要记住的结论" tabindex="-1">简单要记住的结论 <a class="header-anchor" href="#简单要记住的结论" aria-label="Permalink to &quot;简单要记住的结论&quot;">​</a></h3><ul><li>Encoder 适合做分析 做一次就停了</li><li>Decoder 适合做生成 每蹦一个字，都要算一次</li><li>最新的大语言模型，几乎都采用了 <code>Decoder Only</code> 的架构</li></ul><h2 id="linear-softmax" tabindex="-1">Linear &amp; Softmax <a class="header-anchor" href="#linear-softmax" aria-label="Permalink to &quot;Linear &amp; Softmax&quot;">​</a></h2><p>在 Transformer 架构中，<strong>Linear</strong> 和 <strong>Softmax</strong> 是两个关键的数学操作，它们通常出现在网络的末端，特别是在<strong>生成任务</strong>中，如机器翻译或文本摘要。这两个环节共同协作，将模型的内部表示（representation）转化为人类可读的、有意义的输出。</p><p>简单来说：</p><ul><li><strong>Linear (线性层)</strong> 负责将复杂的内部表示，<strong>映射</strong>到一个巨大的、包含所有可能词汇的向量空间中。</li><li><strong>Softmax (归一化指数函数)</strong> 负责将这个向量中的值，<strong>转换</strong>成概率分布，告诉我们每个词被选中的可能性有多大。</li></ul><p>我们继续以一个机器翻译任务为例，假设我们正在将“I like apples”翻译成“我喜欢吃苹果”，并且 Decoder 已经处理完所有步骤，准备生成最后一个词：“苹果”。</p><hr><h3 id="_1-linear-层-从表示到词汇表" tabindex="-1">1. Linear 层：从表示到词汇表 <a class="header-anchor" href="#_1-linear-层-从表示到词汇表" aria-label="Permalink to &quot;1. Linear 层：从表示到词汇表&quot;">​</a></h3><p>经过 Transformer Decoder 的层层处理后，模型已经为下一个要生成的词创建了一个非常复杂的、高维度的向量表示。这个向量包含了源语言（“I like apples”）和已生成的目标语言（“我喜欢吃”）的所有上下文信息。</p><p>这个向量本身没有直接的意义，它无法直接告诉我们下一个词是“苹果”还是“香蕉”。<strong>Linear 层的任务就是解决这个问题</strong>。</p><p>假设我们的词汇表（vocabulary）有 50,000 个词（比如，“我”、“喜欢”、“吃”、“苹果”、“香蕉”……一直到第 50,000 个词）。</p><ul><li><strong>操作</strong>：Linear 层本质上是一个全连接层（fully connected layer），它会对 Decoder 输出的那个向量进行一次<strong>矩阵乘法</strong>。</li><li><strong>输出</strong>：它会把这个高维度的向量，<strong>映射</strong>成一个同样大小为 50,000 的新向量。这个新的向量被称为 <strong>logits 向量</strong>。</li></ul><p><strong>举例</strong>： 如果我们的词汇表大小是 50,000，那么 Linear 层会输出一个长度为 50,000 的向量。这个向量的每一个位置都对应词汇表中的一个词，其值（logits）代表了模型对该词的“偏好”或“倾向性”。</p><p>例如，这个 logits 向量可能看起来像这样：</p><ul><li>...</li><li>logits[&quot;苹果&quot;] = 8.5</li><li>...</li><li>logits[&quot;香蕉&quot;] = 2.1</li><li>...</li><li>logits[&quot;橘子&quot;] = 1.5</li><li>...</li></ul><p><strong>注意</strong>：这些值（8.5, 2.1, 1.5）本身没有直观的意义，它们可以是任何实数，甚至是负数。值越大，代表模型越“倾向于”选择这个词。</p><hr><h3 id="_2-softmax-层-从偏好到概率" tabindex="-1">2. Softmax 层：从偏好到概率 <a class="header-anchor" href="#_2-softmax-层-从偏好到概率" aria-label="Permalink to &quot;2. Softmax 层：从偏好到概率&quot;">​</a></h3><p>Linear 层的输出告诉我们模型的“偏好”，但这不是一个概率分布。为了让这些值变成可解释的概率，并且所有概率加起来等于 1，我们需要使用 <strong>Softmax 函数</strong>。</p><ul><li><strong>操作</strong>：Softmax 会对 Linear 层输出的 logits 向量进行处理。它会先对每个值取指数（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mi>x</mi></msup></mrow><annotation encoding="application/x-tex">e^x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6644em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span>），然后用这个值除以所有值的指数和。</li><li><strong>数学公式</strong>：对于第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 个词，其概率计算为：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mtext>word</mtext><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><msub><mtext>logit</mtext><mi>i</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><msup><mi>e</mi><msub><mtext>logit</mtext><mi>j</mi></msub></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(\text{word}_i) = \frac{e^{\text{logit}_i}}{\sum_{j=1}^{V} e^{\text{logit}_j}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord">word</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.8332em;vertical-align:-1.307em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em;"><span style="top:-2.1288em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8772em;"><span style="top:-3.0911em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">logit</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2052em;"><span style="top:-2.2341em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4048em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">logit</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2052em;"><span style="top:-2.2341em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2659em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.307em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p> 其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> 是词汇表的大小。</li></ul><p><strong>举例</strong>： 继续使用上面 Linear 层的输出。Softmax 会对这些值进行处理：</p><ol><li>对每个 logits 值取指数： <ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mn>8.5</mn></msup><mo>≈</mo><mn>4914.77</mn></mrow><annotation encoding="application/x-tex">e^{8.5} \approx 4914.77</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8.5</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">4914.77</span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mn>2.1</mn></msup><mo>≈</mo><mn>8.17</mn></mrow><annotation encoding="application/x-tex">e^{2.1} \approx 8.17</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2.1</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">8.17</span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mn>1.5</mn></msup><mo>≈</mo><mn>4.48</mn></mrow><annotation encoding="application/x-tex">e^{1.5} \approx 4.48</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1.5</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">4.48</span></span></span></span></li></ul></li><li>计算所有指数值的总和。</li><li>用每个指数值除以总和，得到概率： <ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;苹果&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><mn>8.5</mn></msup><mrow><msup><mi>e</mi><mn>8.5</mn></msup><mo>+</mo><msup><mi>e</mi><mn>2.1</mn></msup><mo>+</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow></mfrac><mo>≈</mo><mn>0.95</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;苹果&quot;}) = \frac{e^{8.5}}{e^{8.5} + e^{2.1} + ...} \approx 0.95</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;</span><span class="mord cjk_fallback">苹果</span><span class="mord">&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.4213em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0179em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">8.5</span></span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2.1</span></span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">...</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">8.5</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.95</span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;香蕉&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><mn>2.1</mn></msup><mrow><msup><mi>e</mi><mn>8.5</mn></msup><mo>+</mo><msup><mi>e</mi><mn>2.1</mn></msup><mo>+</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow></mfrac><mo>≈</mo><mn>0.01</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;香蕉&quot;}) = \frac{e^{2.1}}{e^{8.5} + e^{2.1} + ...} \approx 0.01</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;</span><span class="mord cjk_fallback">香蕉</span><span class="mord">&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.4213em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0179em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">8.5</span></span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2.1</span></span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">...</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2.1</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.01</span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>&quot;橘子&quot;</mtext><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><mn>1.5</mn></msup><mrow><msup><mi>e</mi><mn>8.5</mn></msup><mo>+</mo><msup><mi>e</mi><mn>2.1</mn></msup><mo>+</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow></mfrac><mo>≈</mo><mn>0.005</mn></mrow><annotation encoding="application/x-tex">P(\text{&quot;橘子&quot;}) = \frac{e^{1.5}}{e^{8.5} + e^{2.1} + ...} \approx 0.005</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">&quot;</span><span class="mord cjk_fallback">橘子</span><span class="mord">&quot;</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.4213em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0179em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">8.5</span></span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2.1</span></span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">...</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1.5</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.005</span></span></span></span></li></ul></li></ol><p>最终，Softmax 层的输出是一个<strong>概率分布</strong>，告诉我们模型预测下一个词是每个词的几率。在这个例子中，模型预测下一个词是“苹果”的几率高达 95%。</p><p>最后，模型会根据这个概率分布做出决策，通常是选择概率最高的词（“苹果”），然后将其作为下一个生成的 token，并继续下一个循环。</p><p>总结来说，<strong>Linear 层</strong>负责将模型的内部理解<strong>投射</strong>到词汇表中，而 <strong>Softmax 层</strong>则将这种投射结果<strong>转化为</strong>清晰的、可选择的概率，完成了从抽象表示到具体词汇的最终转换。</p><ul><li><code>softmax 函数</code>也被称为<code>归一化函数</code>，它的作用是将一个向量中的值，映射到一个概率分布上。</li></ul><!--]--></div><!----></div></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/publish/ai/transformer-02" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Previous page</span><span class="title" data-v-e257564d>Transformer 揭秘02</span><!--]--></a></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/publish/ai/neural-network-02" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>神经网络 揭秘02</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>基于 MIT 协议发布</p><p class="copyright" data-v-e315a0ad>Copyright © 2024-present</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about.md\":\"BlILEU08\",\"ai_ai-algorithms-in-different-fields.md\":\"CigS2gut\",\"ai_ai-basic-01.md\":\"4ZZk-kky\",\"ai_ai-images-01.md\":\"B0YhJF74\",\"ai_analytical-ai-01.md\":\"DKQZ0GEM\",\"ai_coze-01.md\":\"Bir6jqWS\",\"ai_embeddings.md\":\"C4uepbqZ\",\"ai_function-calling-vs-mcp.md\":\"BKsLo8q2\",\"ai_function-calling.md\":\"BsaJhe5U\",\"ai_linear-algebra-01.md\":\"DJz6ijx0\",\"ai_linear-algebra-02.md\":\"B8cqrg4_\",\"ai_machine-learning.md\":\"BitPl6Gp\",\"ai_neural-network-01.md\":\"DoDOjPPM\",\"ai_neural-network-02.md\":\"Bo_WUlWb\",\"ai_rag01.md\":\"BKtpY6eW\",\"ai_slm.md\":\"C5s27sB8\",\"ai_time-series-analysis.md\":\"D_EXmSZu\",\"ai_transformer-01.md\":\"ClveA2oS\",\"ai_transformer-02.md\":\"B6LKj5n2\",\"ai_used-car.md\":\"DeLZl6zu\",\"ai_vector-database.md\":\"DSk1xDos\",\"foundation_code.md\":\"DK9KJsHw\",\"foundation_coding-01.md\":\"CqfK0Peu\",\"foundation_data-cleaning.md\":\"DZs6t5zl\",\"foundation_data-science.md\":\"CNDOCre0\",\"foundation_jupyter.md\":\"BAl-XCE_\",\"foundation_probability-theory.md\":\"tbi1Lp98\",\"foundation_python.md\":\"Bw9Qk_yG\",\"foundation_statistics.md\":\"D6aU0Ehq\",\"foundation_wucai-code.md\":\"D4pB5CDc\",\"index.md\":\"Bvro5rc7\",\"other_ai-movie.md\":\"D3LWuPgD\",\"other_deep-search.md\":\"DidQsyzS\",\"other_getting-started.md\":\"D5-zkTrT\",\"other_github-pages.md\":\"D_QCxqUA\",\"other_latex-test.md\":\"dt1ip36x\",\"other_markdown-guide.md\":\"-65R0uff\",\"other_ssh.md\":\"CIN-ryJd\",\"think_ai-benefits-4-pm.md\":\"BaAVegvB\",\"think_junior.md\":\"BvMFzvwF\",\"think_pm.md\":\"C_ktVvln\",\"web_ai-design.md\":\"C10gSzsH\",\"web_cursor-mcp-to-figma.md\":\"D08eoi7Z\",\"web_from-figma-to-mini.md\":\"BkLVvALI\",\"web_hybrid-rendering.md\":\"CCeFoB_z\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"zh-CN\",\"dir\":\"ltr\",\"title\":\"AI时代的技术分享\",\"description\":\"分享技术心得\",\"base\":\"/publish/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"siteTitle\":\"AI时代开发之旅\",\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"文章\",\"link\":\"/list\"},{\"text\":\"关于\",\"link\":\"/about\"}],\"sidebar\":{\"/ai/\":[{\"text\":\"学习笔记\",\"collapsed\":false,\"items\":[{\"text\":\"时间序列分析\",\"link\":\"/ai/time-series-analysis\"},{\"text\":\"二手车预测价格大赛\",\"link\":\"/ai/used-car\"},{\"text\":\"不同领域的 AI 算法\",\"link\":\"/ai/ai-algorithms-in-different-fields\"},{\"text\":\"分析式AI 01\",\"link\":\"/ai/analytical-ai-01\"},{\"text\":\"RAG 01\",\"link\":\"/ai/rag01\"},{\"text\":\"向量数据库\",\"link\":\"/ai/vector-database\"},{\"text\":\"Embeddings\",\"link\":\"/ai/embeddings\"},{\"text\":\"Coze 使用 01\",\"link\":\"/ai/coze-01\"},{\"text\":\"小语言模型的春天到了吗\",\"link\":\"/ai/slm\"},{\"text\":\"AI 生图使用\",\"link\":\"/ai/ai-images-01\"},{\"text\":\"Function Calling vs MCP\",\"link\":\"/ai/function-calling-vs-mcp\"},{\"text\":\"Function Calling 的原始形态\",\"link\":\"/ai/function-calling\"},{\"text\":\"AI 基础 01\",\"link\":\"/ai/ai-basic-01\"},{\"text\":\"Transformer 揭秘02\",\"link\":\"/ai/transformer-02\"},{\"text\":\"Transformer 揭秘01\",\"link\":\"/ai/transformer-01\"},{\"text\":\"神经网络 揭秘02\",\"link\":\"/ai/neural-network-02\"},{\"text\":\"神经网络 揭秘01\",\"link\":\"/ai/neural-network-01\"},{\"text\":\"线性代数基础 02\",\"link\":\"/ai/linear-algebra-02\"},{\"text\":\"线性代数基础 01\",\"link\":\"/ai/linear-algebra-01\"},{\"text\":\"机器学习，深度学习，强化学习\",\"link\":\"/ai/machine-learning\"}]}],\"/foundation/\":[{\"text\":\"foundation\",\"collapsed\":false,\"items\":[{\"text\":\"wucai-code 使用\",\"link\":\"/foundation/wucai-code\"},{\"text\":\"浅谈数据清洗\",\"link\":\"/foundation/data-cleaning\"},{\"text\":\"数据科学与AI应用开发的关系\",\"link\":\"/foundation/data-science\"},{\"text\":\"AI 辅助编程的注意事项\",\"link\":\"/foundation/coding-01\"},{\"text\":\"AI 辅助编程初体验\",\"link\":\"/foundation/code\"},{\"text\":\"Jupyter 使用\",\"link\":\"/foundation/jupyter\"},{\"text\":\"Python 使用基础\",\"link\":\"/foundation/python\"},{\"text\":\"统计学知识\",\"link\":\"/foundation/statistics\"},{\"text\":\"概率论 01\",\"link\":\"/foundation/probability-theory\"}]}],\"/other/\":[{\"text\":\"other\",\"collapsed\":false,\"items\":[{\"text\":\"AI 的深度检索\",\"link\":\"/other/deep-search\"},{\"text\":\"AI 制作长视频实践\",\"link\":\"/other/ai-movie\"},{\"text\":\"使用 GitHub Pages 部署静态网站\",\"link\":\"/other/github-pages\"},{\"text\":\"LaTeX 测试页面\",\"link\":\"/other/latex-test\"},{\"text\":\"Markdown 语法完全指南\",\"link\":\"/other/markdown-guide\"},{\"text\":\"开始使用 VitePress 搭建技术博客\",\"link\":\"/other/getting-started\"},{\"text\":\"ssh 原理图\",\"link\":\"/other/ssh\"}]}],\"/think/\":[{\"text\":\"观察思考\",\"collapsed\":false,\"items\":[{\"text\":\"AI时代初级岗位是否存在\",\"link\":\"/think/junior\"},{\"text\":\"AI 给产品经理带来的帮助\",\"link\":\"/think/ai-benefits-4-pm\"},{\"text\":\"AI草莽时代的产品经理肖像分析\",\"link\":\"/think/pm\"}]}],\"/web/\":[{\"text\":\"前端开发\",\"collapsed\":false,\"items\":[{\"text\":\"Figma MCP 与 Cursor 配合使用\",\"link\":\"/web/cursor-mcp-to-figma\"},{\"text\":\"从 Figma 还原小程序代码\",\"link\":\"/web/from-figma-to-mini\"},{\"text\":\"AI时代设计开发工作流\",\"link\":\"/web/ai-design\"},{\"text\":\"web 网站的混合渲染\",\"link\":\"/web/hybrid-rendering\"}]}]},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/johnyu2023\"},{\"icon\":\"rss\",\"link\":\"/publish/rss.xml\"}],\"footer\":{\"message\":\"基于 MIT 协议发布\",\"copyright\":\"Copyright © 2024-present\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>