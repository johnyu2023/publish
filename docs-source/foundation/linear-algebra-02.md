---
title: 线性代数基础 02
description: 向量和计算
date: 2025-02-16
tags: [Linear Algebra]
---

<BlogPost>

## 1. `标量`和`向量`

### 概念

+ 标量 Scalar - 一个只有**大小**，没有**方向**的量
+ 向量 Vector - 一个既有**大小**，又有**方向**的量

### 乘积 Product - 乘法的结果

### 标量和向量的乘积

标量和向量的乘积在物理、工程、计算机图形学等领域都有非常重要的实际意义，它通常用来改变向量的**大小**或**方向**，但**不改变其方向**。简单来说，就是对一个已有的向量进行**缩放**。

### 点积 Dot Product = 点乘 Scalar product = 表示两个`向量`相乘得到一个`标量`

+ **Dot product** 这个名称来自其运算符号——一个**点**（•），例如 A • B。
+ **Scalar product** 这个名称则强调运算的结果是一个**标量**（scalar）。

### 叉积 Cross Product = 叉乘 Vector product = 表示两个`向量`相乘得到一个`向量`

+ **Cross product** 这个名称来自其运算符号——一个**叉**（×），例如 $A \cdot B$。
+ **Vector product** 这个名称则强调运算的结果是一个**向量**（vector）。

### 点积与叉积的用途

+ 点积主要用来衡量向量之间的投影关系
+ 叉乘主要用来找到与两个向量都垂直的新向量。

## 2. AI 中`点积`的用途

点积在AI领域，尤其是深度学习中，扮演着核心角色。它主要用于解决以下几个问题：

### 1. 向量相似度计算

点积可以衡量两个向量的相似度。在 AI 中，数据（如词语、图像特征、用户偏好等）通常被表示成高维向量。通过计算这些向量的点积，我们可以判断它们在方向上的接近程度：

+ **正值**：两个向量方向相近，代表它们所表示的概念或数据很相似。
+ **负值**：两个向量方向相反，代表它们不相似。
+ **零**：两个向量正交（夹角90度），代表它们不相关。

这种相似度计算在**推荐系统**中尤为常见，例如，通过计算用户向量和电影向量的点积，可以预测用户对某部电影的喜好程度。

### 2. 神经网络中的权重和输入

在神经网络的基本单元——神经元中，点积是核心计算。每个神经元接收来自上一层神经元的输入信号（向量），并将其与自身的权重（另一个向量）进行点积运算。这个点积的结果，加上一个偏置项，构成了神经元的总输入，随后通过激活函数进行非线性转换。

$$\text{总输入} = \text{权重} \cdot \text{输入} + \text{偏置}$$

这个过程实际上是在寻找输入数据中与该神经元“关注”的模式最匹配的部分。点积越大，意味着输入数据与神经元的权重模式越契合。

### 3. 注意力机制（Attention Mechanism）

在现代的自然语言处理（NLP）模型（如 Transformer）中，点积被广泛用于实现“注意力机制”。它通过计算查询（Query）向量和多个键（Key）向量的点积，来衡量查询与每个键之间的相关性。点积结果越大，表明该查询对该键的“注意力”越高，从而赋予其更高的权重。

例如，在机器翻译中，当模型翻译一个单词时，它会通过点积计算当前单词（查询）与源语言句子中所有单词（键）的相似度，以决定“关注”哪些源单词来更好地理解和翻译。

## 3. `向量`与`矩阵`的关系

### 向量（Vector）

想象一下你要记录一个人的体重、身高和年龄。你可以把这些数据写成一列：

$$
\begin{bmatrix}
65 \\
170 \\
30
\end{bmatrix}
$$

这就是一个**向量**。它包含了多个数据点，通常用来表示某个对象的**属性**或在空间中的**位置**。

### 矩阵（Matrix）

现在，假设你要记录三个不同的人（比如甲、乙、丙）的体重、身高和年龄。你可以把这三组数据分别写成三个向量，然后把它们并排放置，形成一个表格：

$$
\begin{bmatrix}
65 & 70 & 80 \\
170 & 175 & 180 \\
30 & 35 & 40
\end{bmatrix}
$$

这就是一个**矩阵**。它是由多个向量组成的，可以看作是**向量的集合**。

---

### 关系总结

+ **向量是矩阵的特例：** 一个只有一列（或一行）的矩阵，就是向量。
+ **矩阵是向量的集合：** 一个矩阵可以看作是由多个列向量（或行向量）组成的。
+ **作用不同：** 向量常用来表示单个对象的数据，而矩阵则常用来表示**多个对象**的数据，或者用来表示**变换**（比如旋转、缩放等操作）。

例如，在计算机图形学中，一个向量可以表示一个点在三维空间中的坐标 $(x, y, z)$，而一个矩阵则可以用来表示一个旋转操作，当你用这个矩阵乘以这个向量时，就可以得到点在旋转后的新坐标。

## 4. `向量`和`数值`的`加减乘除`关系 ，及实际意义

向量和数值之间的加减乘除关系，可以理解为对向量中的每一个元素进行相同的操作。我们通常把数值称为**标量（Scalar）**，因为它只有一个值，没有方向。

### 1. 加法与减法

+ **关系：** 当一个向量和一个标量进行加法或减法运算时，这个标量会与向量中的**每一个元素**进行相应的运算。
+ **例子：**
    假设我们有一个向量，表示你在三天内喝水的杯数：
    $$
    \text{向量} \textbf{A} = \begin{bmatrix} 2 \\ 3 \\ 4 \end{bmatrix} \quad \text{（第一天、第二天、第三天）}
    $$
    如果你决定从第二天开始，每天多喝 **1 杯** 水（这个 **1** 就是标量），那么新的喝水情况可以用加法表示：
    $$
    \begin{bmatrix} 2 \\ 3 \\ 4 \end{bmatrix} + 1 = \begin{bmatrix} 2+1 \\ 3+1 \\ 4+1 \end{bmatrix} = \begin{bmatrix} 3 \\ 4 \\ 5 \end{bmatrix}
    $$
    减法同理，比如你每天少喝 0.5 杯。
+ **实际意义：** 这种运算通常用来对向量中的所有数据进行**平移**或**统一调整**。比如，给所有考试分数都加上一个奖励分，或者对一组温度数据进行校准。

### 2. 乘法与除法

+ **关系：** 当一个向量和一个标量进行乘法或除法运算时，这个标量会与向量中的**每一个元素**进行相应的运算。
+ **例子：**
    假设一个向量表示做一道菜所需的原料克数：
    $$
    \text{向量} \textbf{B} = \begin{bmatrix} 50 \\ 100 \\ 20 \end{bmatrix} \quad \text{（面粉、糖、鸡蛋）}
    $$
    如果你想做 **2 份** 的量（这个 **2** 就是标量），你需要将所有原料的量都乘以 2：
    $$
    2 \times \begin{bmatrix} 50 \\ 100 \\ 20 \end{bmatrix} = \begin{bmatrix} 2 \times 50 \\ 2 \times 100 \\ 2 \times 20 \end{bmatrix} = \begin{bmatrix} 100 \\ 200 \\ 40 \end{bmatrix}
    $$
    除法同理，比如你想把一份菜的量缩小到原来的一半。
+ **实际意义：** 这种运算通常用来对向量中的所有数据进行**缩放**。比如，放大或缩小一张图片中的所有像素值，或者将一组货币单位进行统一转换。

**总结：**
向量和标量的运算是**逐元素（element-wise）**进行的。它使得我们可以方便地对一组数据进行统一的**平移（加减）**或**缩放（乘除）**，这在数据处理、机器学习和图形学中都非常常见。

## 5. `向量`和`向量`的`加减乘除`关系 ，及实际意义

向量和向量之间的加减乘除关系，与向量和数值（标量）的运算不同，它们通常是**逐元素**进行的，但乘法有多种类型，而除法并非标准运算。

### 1. 向量和向量的加法与减法

+ **关系：** 两个相同维度的向量相加或相减，是将它们**对应位置的元素**进行相加或相减，结果是一个新的同维度向量。
+ **例子：**
    假设你有一个向量 $\textbf{A}$ 记录你上周三天的运动时长（单位：小时），以及另一个向量 $\textbf{B}$ 记录你本周三天的运动时长：
    $$\textbf{A} = \begin{bmatrix} 1.5 \\ 1.0 \\ 2.0 \end{bmatrix} \quad \text{（上周一、二、三）}$$
    $$\textbf{B} = \begin{bmatrix} 1.0 \\ 1.5 \\ 1.5 \end{bmatrix} \quad \text{（本周一、二、三）}$$
    如果你想知道这两周每天运动时长的总和，可以进行向量加法：
    $$\textbf{A} + \textbf{B} = \begin{bmatrix} 1.5+1.0 \\ 1.0+1.5 \\ 2.0+1.5 \end{bmatrix} = \begin{bmatrix} 2.5 \\ 2.5 \\ 3.5 \end{bmatrix}$$
+ **实际意义：** 向量加减法通常用来**合并或比较**两组具有相同属性的数据。例如，合并两张购物清单，或者计算两个城市在不同月份的温差。

---

### 2. 向量和向量的乘法

向量乘法主要有两种类型，它们的运算方式和结果都不同：

#### A. 点积（内积，Dot Product）

+ **关系：** 两个同维度向量的点积，是它们**对应元素相乘后的总和**，结果是一个**数值（标量）**。
  + **公式：** $\textbf{A} \cdot \textbf{B} = a_1b_1 + a_2b_2 + \dots + a_nb_n$
+ **例子：**
    假设你有两个向量，一个记录了你购买三种水果的数量，另一个记录了每种水果的价格：
    $$\textbf{数量} = \begin{bmatrix} 5 \\ 3 \\ 2 \end{bmatrix} \quad \text{（苹果、香蕉、橙子）}$$
    $$\textbf{价格} = \begin{bmatrix} 3 \\ 2 \\ 4 \end{bmatrix} \quad \text{（元/个）}$$
    你想计算购买这些水果的总花费，就需要用数量向量和价格向量进行点积：
    $$\textbf{数量} \cdot \textbf{价格} = (5 \times 3) + (3 \times 2) + (2 \times 4) = 15 + 6 + 8 = 29$$
    结果是总花费 **29 元**。
+ **实际意义：** 点积通常用来计算**总和**，比如总成本、总收益，或者在物理学中计算**功**（力向量与位移向量的点积）。它代表了两个向量的“相似”程度或投影关系。

#### B. 叉积（外积，Cross Product）

+ **关系：** 两个三维向量的叉积，结果是一个**新的三维向量**，它**同时垂直于**原来的两个向量。
+ **例子：**
    在物理学中，如果你有一个力向量 $\textbf{F}$ 作用在一个位置向量 $\textbf{r}$ 上，产生的力矩 $\boldsymbol{\tau}$ 就是这两个向量的叉积：
    $$\boldsymbol{\tau} = \textbf{r} \times \textbf{F}$$
    计算出的向量 $\boldsymbol{\tau}$ 的方向表示旋转的方向，大小表示力矩的大小。
+ **实际意义：** 叉积主要用于三维空间，在物理学（如力矩、角动量）、计算机图形学（如计算法向量）和机器人学中有重要应用。

---

### 3. 向量和向量的除法

+ **关系：** **向量除法没有像加法或点积那样的标准数学定义**。在数学中，我们通常不进行“一个向量除以另一个向量”的运算。
+ **但在实际应用中：** 如果要实现类似“除法”的操作，通常是指**逐元素相除**（Element-wise Division），这在编程中很常见，尤其是在数据处理和机器学习中。
+ **例子：**
    假设你有两个向量，一个记录了你一周三天的总跑步距离，另一个记录了这三天你总共跑了多少圈：
    $$\textbf{距离} = \begin{bmatrix} 4000 \\ 5000 \\ 3000 \end{bmatrix} \quad \text{（米）}$$
    $$\textbf{圈数} = \begin{bmatrix} 10 \\ 12.5 \\ 7.5 \end{bmatrix} \quad \text{（圈）}$$
    如果你想计算每天平均每圈跑的距离，可以进行逐元素除法：
    $$\textbf{距离} \div \textbf{圈数} = \begin{bmatrix} 4000/10 \\ 5000/12.5 \\ 3000/7.5 \end{bmatrix} = \begin{bmatrix} 400 \\ 400 \\ 400 \end{bmatrix}$$
+ **实际意义：** 逐元素除法通常用来计算**比率、平均值或归一化**。例如，计算不同产品线的利润率，或者将一组数据点缩放到一个统一的范围。

## 6. 标量、矢量、矩阵和张量

> **标量是数，矢量是箭头，矩阵是表格，张量是它们的高维推广——在现代AI中，一切数据皆可表示为张量。**

+ 在数学、物理和计算机科学（尤其是机器学习）中，**标量、矢量（向量）、矩阵和张量**是描述数据结构和线性关系的核心概念。它们本质上是**多维数组的不同形式**，按“阶数”（或“秩”，rank）区分。

---

### 一、基本概念：按“阶数”（维度数）分类

| 名称   | 阶数（Rank） | 数学表示        | 形状示例       | 通俗理解             |
|--------|--------------|------------------|----------------|----------------------|
| **标量** | 0 阶         | 一个数           | `5`, `-3.14`   | 没有方向的单一数值    |
| **矢量（向量）** | 1 阶         | 一维数组         | `[1, 2, 3]`    | 有大小和方向的量      |
| **矩阵** | 2 阶         | 二维数组         | 2×3 表格       | 线性变换或数据表格    |
| **张量** | ≥3 阶        | 三维及以上数组   | 3×4×5 立方体   | 高维数据的统一表示    |

> ✅ **张量是矢量和矩阵的推广**：标量（0阶张量）、矢量（1阶张量）、矩阵（2阶张量）都是张量的特例。

---

### 二、详细解释

#### 1. **标量（Scalar）** — 0 阶张量
- 就是一个**单独的数**，没有方向。
- 例子：温度 `25°C`、质量 `5 kg`、学习率 `0.01`。
- 在编程中：`x = 3.14`

---

#### 2. **矢量（Vector）** — 1 阶张量
- 是**一维数组**，既有**大小**又有**方向**（在几何中）。
- 在线性代数中，通常默认是**列向量**（n×1 矩阵）：
  $$
  \mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix}
  $$
- 例子：
  - 物理：速度矢量 `(3 m/s, 4 m/s)`
  - 机器学习：词嵌入 `[0.2, -1.3, 0.8]`
- 在编程中（NumPy）：`v = np.array([1, 2, 3])` → shape: `(3,)`

> 🔸 注意：虽然行向量（1×n）也存在，但在数学中“向量”通常指列向量。

---

#### 3. **矩阵（Matrix）** — 2 阶张量
- 是**二维数组**，由行和列组成。
- 用于表示**线性变换**、**方程组系数**、**图像像素**等。
- 例子：
  $$
  A = \begin{bmatrix}
  1 & 2 & 3 \\
  4 & 5 & 6
  \end{bmatrix}
  \quad \text{（2 行 3 列）}
  $$
- 应用：
  - 线性变换：`y = Ax`（将向量 x 映射为 y）
  - 图像：灰度图可表示为 H×W 矩阵
- 编程中：`A = np.array([[1,2,3], [4,5,6]])` → shape: `(2, 3)`

---

#### 4. **张量（Tensor）** — 3 阶及以上
- 是**多维数组的统称**，阶数 ≥ 3。
- 名称来自**张量分析**（微分几何、广义相对论），但在深度学习中泛指任意维度的数组。
- 例子：
  - **3 阶张量**：彩色图像（高 × 宽 × 通道）→ `(224, 224, 3)`
  - **4 阶张量**：一批彩色图像（样本数 × 高 × 宽 × 通道）→ `(32, 224, 224, 3)`
  - **物理中的应力张量**：描述材料内部受力状态（3×3×3）

- 编程中（PyTorch/TensorFlow/NumPy）：
  ```python
  # 4D 张量：32张 64x64 的RGB图像
  images = np.random.rand(32, 64, 64, 3)  # shape: (32, 64, 64, 3)
  ```

> 💡 在深度学习框架中，“Tensor” 就是**任意维度的数组**，包括标量、向量、矩阵。

---

### 三、关系总结（包含关系）

```
张量（Tensor）
├── 0 阶张量：标量（Scalar）
├── 1 阶张量：矢量（Vector）
├── 2 阶张量：矩阵（Matrix）
└── 3+ 阶张量：高维张量（如图像批次、视频、3D医学影像等）
```

> ✅ 所以：**所有标量、矢量、矩阵都是张量，但张量不一定是矩阵**。

---

### 四、应用场景对比

| 领域         | 标量       | 矢量           | 矩阵               | 高阶张量               |
|--------------|------------|----------------|--------------------|------------------------|
| 物理         | 质量、温度 | 速度、力       | 转动惯量           | 应力、应变张量         |
| 机器学习     | 损失值     | 特征向量       | 权重矩阵、协方差矩阵 | 图像、视频、嵌入批次   |
| 计算机图形   | 缩放因子   | 顶点坐标       | 变换矩阵（旋转/投影）| —                      |
| 数据科学     | 统计量     | 样本特征       | 数据表（样本×特征） | 多模态数据（如时空数据）|

## 7. 2x3 的数组

在 NumPy 中，**“2×3 的数组”** 指的是一个具有 **2 行（rows）和 3 列（columns）** 的二维数组，也就是一个 **2×3 的矩阵**。

---

### ✅ 含义解释：
- **2 行**：数组有 2 个子数组（或 2 个“内层”列表）
- **3 列**：每个子数组包含 3 个元素
- 总共包含 `2 × 3 = 6` 个元素

例如：
```python
[[1, 2, 3],
 [4, 5, 6]]
```
这就是一个典型的 2×3 数组。

---

### ✅ 在 NumPy 中创建 2×3 数组的方法

#### 方法 1：用 `np.array()` 从 Python 列表创建
```python
import numpy as np

arr = np.array([[1, 2, 3],
                [4, 5, 6]])

print(arr)
print("Shape:", arr.shape)  # 输出: (2, 3)
```

#### 方法 2：用 `np.zeros()` 创建全 0 的 2×3 数组
```python
arr = np.zeros((2, 3))
print(arr)
# 输出:
# [[0. 0. 0.]
#  [0. 0. 0.]]
```

#### 方法 3：用 `np.ones()` 创建全 1 的 2×3 数组
```python
arr = np.ones((2, 3))
print(arr)
```

#### 方法 4：用 `np.full()` 创建指定值的数组
```python
arr = np.full((2, 3), 7)
print(arr)
# 输出:
# [[7 7 7]
#  [7 7 7]]
```

#### 方法 5：用 `np.arange()` + `reshape()` 创建连续数字
```python
arr = np.arange(1, 7).reshape(2, 3)
print(arr)
# 输出:
# [[1 2 3]
#  [4 5 6]]
```

> ⚠️ 注意：`reshape(2, 3)` 要求总元素数为 6，所以 `arange(1, 7)`（生成 1~6）正好匹配。

---

### 🔍 验证形状
你可以用 `.shape` 属性查看数组维度：
```python
print(arr.shape)  # 输出: (2, 3)
```
- 第一个数字 `2`：行数（axis=0）
- 第二个数字 `3`：列数（axis=1）

---

### 💡 小贴士
- 在 NumPy 中，**“数组”（array）是通用术语**，2D 数组就等价于数学中的**矩阵**。
- 虽然 NumPy 也有 `np.matrix` 类型，但**官方已不推荐使用**，建议统一用 `np.array`。


</BlogPost>
