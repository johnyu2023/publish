---
title: TensorFlow 基础
description: TensorFlow
date: 2025-10-07
tags: [TensorFlow]
---

<BlogPost>

## TensorFlow中的`计算图`

### 含义解释

**计算图 - Computation graph** 是一种用图（graph）结构来表示计算过程的方式。在这个图中：

- **节点（Nodes）** 表示操作（operations，简称 ops），比如加法、乘法、矩阵运算、激活函数等。
- **边（Edges）** 表示数据（通常是张量，tensors）在操作之间的流动。

### 举个简单例子

假设我们要计算 `z = (a + b) * c`，那么对应的 computation graph 可能如下：

``` text
   a ----\
          + ----\
   b ----/       * ----> z
                 /
   c -----------/
```

- `+` 和 `*` 是操作节点（ops）。
- `a`、`b`、`c` 是输入张量（tensors）。
- 中间的加法结果和最终的乘法结果也是张量，沿着边传递。

### 在 TensorFlow 中的作用

- 在 **TensorFlow 1.x** 中，计算图是核心概念。用户先构建一个静态图（static graph），然后在会话（Session）中运行它。
- 在 **TensorFlow 2.x** 中，默认启用了 **eager execution**（动态执行），使得代码像普通 Python 一样立即执行，但 TensorFlow 仍然支持通过 `@tf.function` 装饰器将 Python 函数自动转换为高效的 computation graph，以提升性能和便于部署。

### 计算图的优点

- 高效执行：图优化（如算子融合）可提升性能。
- 跨平台支持：可导出到移动端、嵌入式设备等。

###

``` python
a = tf.constant(2)
b = tf.constant(3)
c = a + b # 构建计算图，但不立即计算
```

- 计算图可加速计算，图模式下运算在C++层执行，避免 Python 循环的慢速交互。
- 能并行化优化：自动调度无依赖的节点并行计算（如 GPU/多核CPU）。

## 张量

> 张量（Tensor） 是 TensorFlow 中的核心数据结构，可以理解为多维数组。

- 标量（0维）：单个数值（如 3.0）。
- 向量（1维）：一维数组（如 [1, 2, 3]）。
- 矩阵（2维）：二维表格（如 [[1, 2], [3, 4]]）。
- 高维张量：如 RGB 图片（3维：[高度, 宽度, 通道]）。

张量存储数据，并在计算图中流动，支持自动求导和 GPU 加速。

## 三维数组

### 1. 一维数组（1D Array）  

**形状（shape）**：`(n,)`，只有一个维度。

**例子**：一个班级5个学生的语文成绩  

```python
scores = [85, 92, 78, 96, 88]
```

- 这是一个**列表**，在 NumPy 或 TensorFlow 中就是一个**一维数组**。
- 可以理解为**一条线**上的数据。
- 访问方式：`scores[0]` 表示第一个学生的成绩（85）。

✅ **现实类比**：一排座位上坐着5个人，每个人有一个分数。

---

### 2. 二维数组（2D Array）  

**形状（shape）**：`(m, n)`，有行和列两个维度。

**例子**：3个学生，每人有3门课的成绩（语文、数学、英语）  

```python
grades = [
    [85, 90, 88],   # 学生1
    [78, 92, 85],   # 学生2
    [96, 87, 90]    # 学生3
]
```

- 这是一个**表格**（类似 Excel 表格）。
- 第一维是“学生”，第二维是“科目”。
- 访问方式：`grades[1][2]` 表示第2个学生（索引1）的第3门课（索引2）成绩 → 85。

✅ **现实类比**：一个 3×3 的棋盘，每个格子存一个数字。

> 💡 在机器学习中，**数据集通常表示为二维数组**：每一行是一个样本，每一列是一个特征。

---

### 3. 三维数组（3D Array）  

**形状（shape）**：`(a, b, c)`，有三个维度。

**例子1：多个学生的多天成绩记录**  
假设我们记录2个学生，连续3天，每天考2门课：

```python
records = [
    [               # 学生0
        [85, 90],   # 第1天：语文85，数学90
        [88, 92],   # 第2天
        [80, 85]    # 第3天
    ],
    [               # 学生1
        [78, 88],   # 第1天
        [82, 90],   # 第2天
        [85, 87]    # 第3天
    ]
]
```

- 形状是 `(2, 3, 2)`：
  - 第1维：2个学生
  - 第2维：3天
  - 第3维：每天2门课成绩

访问：`records[0][1][0]` → 学生0、第2天（索引1）、第1门课（索引0）→ 88。

---

**例子2：彩色图像（更常见于深度学习）**  
一张 64×64 像素的彩色图片，有 RGB 三个颜色通道：

- 高度：64 像素  
- 宽度：64 像素  
- 通道数：3（红、绿、蓝）

所以图像就是一个 **三维数组**，形状为 `(64, 64, 3)`。

- `image[10][20][0]` 表示第10行、第20列像素的 **红色通道** 值（0~255）。

✅ **现实类比**：像一本“书”：

- 每一页是一个二维矩阵（比如一张灰度图），
- 三页叠在一起（R、G、B）就组成一张彩色图。

---

### 总结对比

| 维度 | 形状示例     | 类比             | 常见用途                     |
|------|--------------|------------------|------------------------------|
| 1D   | `(5,)`       | 一排数字         | 单个样本的特征、时间序列     |
| 2D   | `(3, 4)`     | 表格、矩阵       | 数据集（样本×特征）          |
| 3D   | `(2, 3, 4)`  | 一叠表格 / 书    | 多样本时间序列、彩色图像     |

在 TensorFlow/NumPy 中，数组的维度（`ndim`）和形状（`shape`）可以通过 `.ndim` 和 `.shape` 查看。

## 传统的计算图（Graph）+ 会话（Session）模式（TensorFlow 1.x）

这个问题问得非常好！要理解 TensorFlow 2.x 的改进，首先需要了解 **TensorFlow 1.x 中“计算图 + 会话”模式**到底是什么。

---

### 一、什么是“传统的计算图（Graph）+ 会话（Session）模式”？

这是 **TensorFlow 1.x 的默认编程范式**，其核心思想是：

> **先定义，后执行** —— 先构建一个完整的计算流程（称为“计算图”），然后再通过“会话（Session）”来运行这个图。

#### 1. **计算图（Graph）**

- 计算图是一个**描述计算过程的静态结构**，由节点（操作，如加法、乘法）和边（数据，即张量）组成。
- 它**不立即执行计算**，只是记录“要做什么”。
- 例如：`c = a + b` 在图中只是一个“加法操作节点”，并不会立刻算出结果。

#### 2. **会话（Session）**

- 会话是**执行计算图的环境**。
- 只有在 `Session.run()` 被调用时，图中的计算才会真正发生。
- 你需要显式地启动会话，并告诉它“运行哪个节点”。

---

### 二、TensorFlow 1.x 的典型代码示例

```python
import tensorflow as tf

# 1. 构建计算图（此时不计算！）
a = tf.constant(2)
b = tf.constant(3)
c = a + b  # 这只是一个图中的操作节点

# 2. 启动会话并执行
with tf.Session() as sess:
    result = sess.run(c)  # 真正计算 c 的值
    print(result)  # 输出 5
```

🔍 关键点：

- `a + b` **不会立即输出 5**，它只是往图里加了一个加法操作。
- 必须通过 `sess.run(c)` 才能拿到结果。
- 如果想调试中间变量，非常困难（不能直接 `print(a)` 看值，只能 `sess.run(a)`）。

---

### 三、这种模式的问题

1. **学习曲线陡峭**：新手很难理解“为什么写了代码却不执行？”
2. **调试困难**：无法使用常规的 Python 调试工具（如 print、断点）。
3. **代码冗长**：每次都要写 `with tf.Session(): ...`。
4. **不符合 Python 直觉**：Python 是“写一行，执行一行”，而 TF 1.x 是“先写完所有，再统一执行”。

---

### 四、TensorFlow 2.x 的改进

为了解决上述问题，TF 2.x 做了以下改变：

| 特性 | TensorFlow 1.x | TensorFlow 2.x |
|------|----------------|----------------|
| 默认执行模式 | 图模式（需 Session） | **即时执行（Eager Execution）** |
| 调试体验 | 困难 | 像普通 Python 一样简单 |
| 性能优化 | 依赖静态图 | 通过 `@tf.function` **按需构建图** |

✅ **核心思想**：

- **开发时**：用 Eager 模式，像写 NumPy 一样写代码，随时调试。
- **部署时**：用 `@tf.function` 自动将函数转为高效计算图，保留图的性能优势。

```python
# TF 2.x：默认即时执行
a = tf.constant(2)
b = tf.constant(3)
print(a + b)  # 直接输出结果！

# 需要高性能时，加个装饰器变成图
@tf.function
def fast_add(x, y):
    return x + y
```

## 图与会话的现代应用（TensorFlow 2.x）

> 在 TensorFlow 2.x 中，传统的 计算图（Graph）+ 会话（Session） 模式被大幅简化，但仍保留核心优化能力，主要通过 tf.function 和 即时执行（Eager Execution） 结合使用。

- TF 1.x：需显式构建图和会话，适合高性能场景。
- TF 2.x：默认即时执行，tf.function实现图模式加速。

### tf.function：自动构建计算图

作用：将普通 Python 函数转换为 TensorFlow 计算图，提升执行效率（如减少 Python 调用开销）。

```python
@tf.function # 装饰器，自动编译为计算图
def add_and_multiply(x, y):
return x + y, x * y
result = add_and_multiply(2, 3) # 首次调用时构建图，后续高效执行
```

### 即时执行（Eager Execution）

- Tensorflow2.0默认模式，一种交互式编程模式，让代码像普通 Python 程序一样逐行运行（类似NumPy），无需先构建计算图，便于调试。

``` python
a = tf.constant(2)
b = tf.constant(3)
print(a + b) # 直接输出 5，无需 Session
```

- 与图的协作：
  - 调试时：用即时执行快速验证逻辑。
  - 部署时：用 @tf.function 转换为图模式提升性能。

</BlogPost>
