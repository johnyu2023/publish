---
title: LLM 的训练
description: LLM 都是如何训练出来的
date: 2025-12-05
tags: [LLM，训练]
---

::: blog-post

> 站在大模型外部看它的训练流程，它的训练流程可以分为几个阶段？

## 常见的训练阶段划分

截至 2025 年公开的技术资料（特别是关于 **Llama 3** 的后训练方法），我们可以整理出如下对比表格，并进行简要分析。

### **GPT 与 Llama 训练流程对比**

| 阶段 | GPT（典型流程） | Llama（以 Llama 3 为代表） | 说明 |
|------|------------------|----------------------------|------|
| 1. 预训练（Pretrain） | ✅ | ✅ | 两者都从大规模无监督语料中学习语言建模能力，这是基础。 |
| 2. 监督微调（SFT） | ✅（第2步） | ✅（在后训练中多次出现，如第4步） | GPT 在早期就引入人类标注指令数据进行对齐；Llama 3 的 SFT 是**迭代式后训练的一部分**，常与偏好学习交替进行 。 |
| 3. 奖励模型（Reward Model） | ✅（第3步） | ✅（第2步） | 两者都训练一个独立的 RM 来评估生成结果的质量。但 Llama 更强调**多轮 RM 更新**以适应迭代优化 。 |
| 4. 强化学习优化 | ✅（PPO） | ❌（未用 PPO） | GPT 系列（如 InstructGPT）依赖 **PPO** 进行策略梯度更新；Llama 3 **放弃了 PPO**，因其训练不稳定且计算开销大。 |
| 5. 采样/生成策略 | — | ✅（Rejection Sampling，第3步） | Llama 使用**拒绝采样**：用当前策略生成多个回答，由 RM 选出最佳样本用于后续训练 。GPT 通常不显式使用此步骤。 |
| 6. 偏好优化方法 | — | ✅（DPO，第5步） | Llama 3 采用 **DPO**（Direct Preference Optimization），直接从偏好数据优化策略，**无需训练奖励模型或使用强化学习**，更稳定高效 。 |

> 注：Llama 3 的后训练是**多轮迭代**的，每轮通常包含：**Reward Modeling → Rejection Sampling → SFT → DPO**，共进行约 6 轮 。

---

### **简要分析**

1. **GPT 路线**：  
   - 采用经典的 **“SFT + RM + PPO”** 三段式对齐流程，源自 InstructGPT。  
   - 优点是理论成熟；缺点是 **PPO 训练复杂、不稳定、对超参敏感**，且需要大量计算资源。

2. **Llama 路线**（尤其是 Llama 3）：  
   - 采用**迭代式后训练**，融合多种对齐技术。  
   - **放弃 PPO**，转而使用 **Rejection Sampling + DPO**，提升训练稳定性和数据效率。  
   - DPO 可直接从人类偏好数据中优化模型，**绕过奖励模型的显式训练和强化学习的复杂性**，更适合大规模开源模型的部署 。

3. **趋势对比**：  
   - GPT 流程代表早期对齐范式，适合闭源、资源充足的商业模型。  
   - Llama 的流程反映**2024–2025 年开源社区的主流演进方向**：**简化 RL、提升可复现性、强调迭代优化与数据质量**。

:::
