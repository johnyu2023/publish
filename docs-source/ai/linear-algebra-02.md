---
title: 线性代数基础 02
description: 向量和计算
date: 2025-02-16
tags: [Linear Algebra]
---

<BlogPost>

## 1. `标量`和`向量`

### 概念

+ 标量 Scalar - 一个只有**大小**，没有**方向**的量
+ 向量 Vector - 一个既有**大小**，又有**方向**的量

### 乘积 Product - 乘法的结果

### 标量和向量的乘积

标量和向量的乘积在物理、工程、计算机图形学等领域都有非常重要的实际意义，它通常用来改变向量的**大小**或**方向**，但**不改变其方向**。简单来说，就是对一个已有的向量进行**缩放**。

### 点积 Dot Product = 点乘 Scalar product = 表示两个`向量`相乘得到一个`标量`

+ **Dot product** 这个名称来自其运算符号——一个**点**（•），例如 A • B。
+ **Scalar product** 这个名称则强调运算的结果是一个**标量**（scalar）。

### 叉积 Cross Product = 叉乘 Vector product = 表示两个`向量`相乘得到一个`向量`

+ **Cross product** 这个名称来自其运算符号——一个**叉**（×），例如 $A \cdot B$。
+ **Vector product** 这个名称则强调运算的结果是一个**向量**（vector）。

### 点积与叉积的用途

+ 点积主要用来衡量向量之间的投影关系
+ 叉乘主要用来找到与两个向量都垂直的新向量。

## 2. AI 中`点积`的用途

点积在AI领域，尤其是深度学习中，扮演着核心角色。它主要用于解决以下几个问题：

### 1. 向量相似度计算

点积可以衡量两个向量的相似度。在 AI 中，数据（如词语、图像特征、用户偏好等）通常被表示成高维向量。通过计算这些向量的点积，我们可以判断它们在方向上的接近程度：

+ **正值**：两个向量方向相近，代表它们所表示的概念或数据很相似。
+ **负值**：两个向量方向相反，代表它们不相似。
+ **零**：两个向量正交（夹角90度），代表它们不相关。

这种相似度计算在**推荐系统**中尤为常见，例如，通过计算用户向量和电影向量的点积，可以预测用户对某部电影的喜好程度。

### 2. 神经网络中的权重和输入

在神经网络的基本单元——神经元中，点积是核心计算。每个神经元接收来自上一层神经元的输入信号（向量），并将其与自身的权重（另一个向量）进行点积运算。这个点积的结果，加上一个偏置项，构成了神经元的总输入，随后通过激活函数进行非线性转换。

$$\text{总输入} = \text{权重} \cdot \text{输入} + \text{偏置}$$

这个过程实际上是在寻找输入数据中与该神经元“关注”的模式最匹配的部分。点积越大，意味着输入数据与神经元的权重模式越契合。

### 3. 注意力机制（Attention Mechanism）

在现代的自然语言处理（NLP）模型（如 Transformer）中，点积被广泛用于实现“注意力机制”。它通过计算查询（Query）向量和多个键（Key）向量的点积，来衡量查询与每个键之间的相关性。点积结果越大，表明该查询对该键的“注意力”越高，从而赋予其更高的权重。

例如，在机器翻译中，当模型翻译一个单词时，它会通过点积计算当前单词（查询）与源语言句子中所有单词（键）的相似度，以决定“关注”哪些源单词来更好地理解和翻译。

## 3. `向量`与`矩阵`的关系

### 向量（Vector）

想象一下你要记录一个人的体重、身高和年龄。你可以把这些数据写成一列：

$$
\begin{bmatrix}
65 \\
170 \\
30
\end{bmatrix}
$$

这就是一个**向量**。它包含了多个数据点，通常用来表示某个对象的**属性**或在空间中的**位置**。

### 矩阵（Matrix）

现在，假设你要记录三个不同的人（比如甲、乙、丙）的体重、身高和年龄。你可以把这三组数据分别写成三个向量，然后把它们并排放置，形成一个表格：

$$
\begin{bmatrix}
65 & 70 & 80 \\
170 & 175 & 180 \\
30 & 35 & 40
\end{bmatrix}
$$

这就是一个**矩阵**。它是由多个向量组成的，可以看作是**向量的集合**。

---

### 关系总结

+ **向量是矩阵的特例：** 一个只有一列（或一行）的矩阵，就是向量。
+ **矩阵是向量的集合：** 一个矩阵可以看作是由多个列向量（或行向量）组成的。
+ **作用不同：** 向量常用来表示单个对象的数据，而矩阵则常用来表示**多个对象**的数据，或者用来表示**变换**（比如旋转、缩放等操作）。

例如，在计算机图形学中，一个向量可以表示一个点在三维空间中的坐标 $(x, y, z)$，而一个矩阵则可以用来表示一个旋转操作，当你用这个矩阵乘以这个向量时，就可以得到点在旋转后的新坐标。

## `向量`和`数值`的`加减乘除`关系 ，及实际意义

向量和数值之间的加减乘除关系，可以理解为对向量中的每一个元素进行相同的操作。我们通常把数值称为**标量（Scalar）**，因为它只有一个值，没有方向。

### 1. 加法与减法

+ **关系：** 当一个向量和一个标量进行加法或减法运算时，这个标量会与向量中的**每一个元素**进行相应的运算。
+ **例子：**
    假设我们有一个向量，表示你在三天内喝水的杯数：
    $$
    \text{向量} \textbf{A} = \begin{bmatrix} 2 \\ 3 \\ 4 \end{bmatrix} \quad \text{（第一天、第二天、第三天）}
    $$
    如果你决定从第二天开始，每天多喝 **1 杯** 水（这个 **1** 就是标量），那么新的喝水情况可以用加法表示：
    $$
    \begin{bmatrix} 2 \\ 3 \\ 4 \end{bmatrix} + 1 = \begin{bmatrix} 2+1 \\ 3+1 \\ 4+1 \end{bmatrix} = \begin{bmatrix} 3 \\ 4 \\ 5 \end{bmatrix}
    $$
    减法同理，比如你每天少喝 0.5 杯。
+ **实际意义：** 这种运算通常用来对向量中的所有数据进行**平移**或**统一调整**。比如，给所有考试分数都加上一个奖励分，或者对一组温度数据进行校准。

### 2. 乘法与除法

+ **关系：** 当一个向量和一个标量进行乘法或除法运算时，这个标量会与向量中的**每一个元素**进行相应的运算。
+ **例子：**
    假设一个向量表示做一道菜所需的原料克数：
    $$
    \text{向量} \textbf{B} = \begin{bmatrix} 50 \\ 100 \\ 20 \end{bmatrix} \quad \text{（面粉、糖、鸡蛋）}
    $$
    如果你想做 **2 份** 的量（这个 **2** 就是标量），你需要将所有原料的量都乘以 2：
    $$
    2 \times \begin{bmatrix} 50 \\ 100 \\ 20 \end{bmatrix} = \begin{bmatrix} 2 \times 50 \\ 2 \times 100 \\ 2 \times 20 \end{bmatrix} = \begin{bmatrix} 100 \\ 200 \\ 40 \end{bmatrix}
    $$
    除法同理，比如你想把一份菜的量缩小到原来的一半。
+ **实际意义：** 这种运算通常用来对向量中的所有数据进行**缩放**。比如，放大或缩小一张图片中的所有像素值，或者将一组货币单位进行统一转换。

**总结：**
向量和标量的运算是**逐元素（element-wise）**进行的。它使得我们可以方便地对一组数据进行统一的**平移（加减）**或**缩放（乘除）**，这在数据处理、机器学习和图形学中都非常常见。

## `向量`和`向量`的`加减乘除`关系 ，及实际意义

向量和向量之间的加减乘除关系，与向量和数值（标量）的运算不同，它们通常是**逐元素**进行的，但乘法有多种类型，而除法并非标准运算。

### 1. 向量和向量的加法与减法

+ **关系：** 两个相同维度的向量相加或相减，是将它们**对应位置的元素**进行相加或相减，结果是一个新的同维度向量。
+ **例子：**
    假设你有一个向量 $\textbf{A}$ 记录你上周三天的运动时长（单位：小时），以及另一个向量 $\textbf{B}$ 记录你本周三天的运动时长：
    $$\textbf{A} = \begin{bmatrix} 1.5 \\ 1.0 \\ 2.0 \end{bmatrix} \quad \text{（上周一、二、三）}$$
    $$\textbf{B} = \begin{bmatrix} 1.0 \\ 1.5 \\ 1.5 \end{bmatrix} \quad \text{（本周一、二、三）}$$
    如果你想知道这两周每天运动时长的总和，可以进行向量加法：
    $$\textbf{A} + \textbf{B} = \begin{bmatrix} 1.5+1.0 \\ 1.0+1.5 \\ 2.0+1.5 \end{bmatrix} = \begin{bmatrix} 2.5 \\ 2.5 \\ 3.5 \end{bmatrix}$$
+ **实际意义：** 向量加减法通常用来**合并或比较**两组具有相同属性的数据。例如，合并两张购物清单，或者计算两个城市在不同月份的温差。

---

### 2. 向量和向量的乘法

向量乘法主要有两种类型，它们的运算方式和结果都不同：

#### A. 点积（内积，Dot Product）

+ **关系：** 两个同维度向量的点积，是它们**对应元素相乘后的总和**，结果是一个**数值（标量）**。
  + **公式：** $\textbf{A} \cdot \textbf{B} = a_1b_1 + a_2b_2 + \dots + a_nb_n$
+ **例子：**
    假设你有两个向量，一个记录了你购买三种水果的数量，另一个记录了每种水果的价格：
    $$\textbf{数量} = \begin{bmatrix} 5 \\ 3 \\ 2 \end{bmatrix} \quad \text{（苹果、香蕉、橙子）}$$
    $$\textbf{价格} = \begin{bmatrix} 3 \\ 2 \\ 4 \end{bmatrix} \quad \text{（元/个）}$$
    你想计算购买这些水果的总花费，就需要用数量向量和价格向量进行点积：
    $$\textbf{数量} \cdot \textbf{价格} = (5 \times 3) + (3 \times 2) + (2 \times 4) = 15 + 6 + 8 = 29$$
    结果是总花费 **29 元**。
+ **实际意义：** 点积通常用来计算**总和**，比如总成本、总收益，或者在物理学中计算**功**（力向量与位移向量的点积）。它代表了两个向量的“相似”程度或投影关系。

#### B. 叉积（外积，Cross Product）

+ **关系：** 两个三维向量的叉积，结果是一个**新的三维向量**，它**同时垂直于**原来的两个向量。
+ **例子：**
    在物理学中，如果你有一个力向量 $\textbf{F}$ 作用在一个位置向量 $\textbf{r}$ 上，产生的力矩 $\boldsymbol{\tau}$ 就是这两个向量的叉积：
    $$\boldsymbol{\tau} = \textbf{r} \times \textbf{F}$$
    计算出的向量 $\boldsymbol{\tau}$ 的方向表示旋转的方向，大小表示力矩的大小。
+ **实际意义：** 叉积主要用于三维空间，在物理学（如力矩、角动量）、计算机图形学（如计算法向量）和机器人学中有重要应用。

---

### 3. 向量和向量的除法

+ **关系：** **向量除法没有像加法或点积那样的标准数学定义**。在数学中，我们通常不进行“一个向量除以另一个向量”的运算。
+ **但在实际应用中：** 如果要实现类似“除法”的操作，通常是指**逐元素相除**（Element-wise Division），这在编程中很常见，尤其是在数据处理和机器学习中。
+ **例子：**
    假设你有两个向量，一个记录了你一周三天的总跑步距离，另一个记录了这三天你总共跑了多少圈：
    $$\textbf{距离} = \begin{bmatrix} 4000 \\ 5000 \\ 3000 \end{bmatrix} \quad \text{（米）}$$
    $$\textbf{圈数} = \begin{bmatrix} 10 \\ 12.5 \\ 7.5 \end{bmatrix} \quad \text{（圈）}$$
    如果你想计算每天平均每圈跑的距离，可以进行逐元素除法：
    $$\textbf{距离} \div \textbf{圈数} = \begin{bmatrix} 4000/10 \\ 5000/12.5 \\ 3000/7.5 \end{bmatrix} = \begin{bmatrix} 400 \\ 400 \\ 400 \end{bmatrix}$$
+ **实际意义：** 逐元素除法通常用来计算**比率、平均值或归一化**。例如，计算不同产品线的利润率，或者将一组数据点缩放到一个统一的范围。

</BlogPost>
